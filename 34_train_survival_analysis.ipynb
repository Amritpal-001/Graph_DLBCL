{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "from os import path as osp\n",
    "\n",
    "import torch\n",
    "from tiatoolbox.utils.misc import select_device\n",
    "import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import json\n",
    "import glob\n",
    "\n",
    "# from src.intensity import add_features_and_create_new_dicts\n",
    "\n",
    "from src.featureextraction import get_cell_features, add_features_and_create_new_dicts\n",
    "from src.train import stratified_split, recur_find_ext, run_once, rm_n_mkdir ,reset_logging\n",
    "from src.graph_construct import create_graph_with_pooled_patch_nodes, get_pids_labels_for_key, create_graph_with_pooled_patch_nodes_with_survival_data\n",
    "\n",
    "\n",
    "ON_GPU = False\n",
    "device = select_device(on_gpu=ON_GPU)\n",
    "\n",
    "SEED = 5\n",
    "random.seed(SEED)\n",
    "rng = np.random.default_rng(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "\n",
    "\n",
    "BASEDIR = '/home/amrit/data/proj_data/MLG_project/DLBCL-Morph'\n",
    "\n",
    "\n",
    "STAIN = 'MYC'\n",
    "# STAIN = 'BCL2'\n",
    "# STAIN = 'HE'\n",
    "\n",
    "FIDIR = f'{BASEDIR}/outputs'\n",
    "CLINPATH = f'{BASEDIR}/clinical_data_cleaned.csv'\n",
    "ANNPATH = f'{BASEDIR}/annotations_clean.csv'\n",
    "FEATSDIR = f'{BASEDIR}/outputs/files/{STAIN}'\n",
    "FEATSCALERPATH = f\"{FEATSDIR}/0_feat_scaler.npz\"\n",
    "PATCH_SIZE = 224\n",
    "OUTPUT_SIZE = PATCH_SIZE*8\n",
    "\n",
    "WORKSPACE_DIR = Path(BASEDIR)\n",
    "# GRAPH_DIR = WORKSPACE_DIR / f\"graphs{STAIN}\" \n",
    "# LABELS_PATH = WORKSPACE_DIR / \"graphs/0_labels.txt\"\n",
    "\n",
    "\n",
    "# Graph construction\n",
    "# PATCH_SIZE = 300\n",
    "SKEW_NOISE = 0.0001\n",
    "MIN_CELLS_PER_PATCH = 10\n",
    "CONNECTIVITY_DISTANCE = 500\n",
    "\n",
    "LABEL_TYPE = 'multilabel' #'OS' #\n",
    "LABEL_TYPE = 'OS' #'OS' #\n",
    "\n",
    "\n",
    "GRAPHSDIR = Path(f'{BASEDIR}/graphs/{STAIN}')\n",
    "LABELSPATH = f'{BASEDIR}/graphs/{STAIN}_labels.json'\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "NUM_NODE_FEATURES = 128\n",
    "NCLASSES = 3\n",
    "\n",
    "TRAIN_DIR = WORKSPACE_DIR / \"training\"\n",
    "SPLIT_PATH = TRAIN_DIR / f\"splits_{STAIN}_{LABEL_TYPE}.dat\"\n",
    "# RUN_OUTPUT_DIR = TRAIN_DIR / f\"session_{STAIN}_{datetime.now().strftime('%m_%d_%H_%M')}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract features: 20 + 4 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read annotation csv, filter, and process intensity features\n",
    "df = pd.read_csv(ANNPATH)\n",
    "\n",
    "df = df[df['stain'] == STAIN]\n",
    "df['area'] = (df['xe'] - df['xs']) *  (df['ye'] - df['ys'])/10000\n",
    "df = df[df['area'] >= 150]  \n",
    "df = df[df['xs']  >=0 ]\n",
    "df = df[df['ys']  >=0 ]\n",
    "df = df[df['xe']  >=0 ]\n",
    "df = df[df['ye']  >=0 ]\n",
    "\n",
    "df = df.reset_index()\n",
    "##########\n",
    "\n",
    "###############\n",
    "# add intensity features\n",
    "start_index = 0\n",
    "end_index = len(df.index)\n",
    "\n",
    "datpaths = []\n",
    "imgpaths = []\n",
    "updatpaths = []\n",
    "\n",
    "# for index in range(start_index, 1):\n",
    "for index in range(start_index, end_index):\n",
    "    df_index = df['index'][index]\n",
    "    patient_id = df['patient_id'][index]\n",
    "    stain = df['stain'][index]\n",
    "    tma_id = df['tma_id'][index]\n",
    "    unique_id = str(patient_id) + '_' + stain + '_' + str(df_index)\n",
    "\n",
    "    img_file_name = f\"{FIDIR}/images/{patient_id}/{patient_id}_{stain}_{tma_id}_{OUTPUT_SIZE}_{df_index}.png\"\n",
    "    dat_file_name = f\"{FIDIR}/files/{stain}/{patient_id}/{df_index}/0.dat\"\n",
    "    updat_file_name = f\"{FIDIR}/files/{stain}/{patient_id}/{df_index}/{unique_id}.dat\"\n",
    "\n",
    "    datpaths.append(dat_file_name)\n",
    "    imgpaths.append(img_file_name)\n",
    "    updatpaths.append(updat_file_name)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updatpaths[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16 minutes\n",
    "# shutil.rmtree(FEATSDIR)\n",
    "# if not osp.exists(FEATSDIR):\n",
    "#     os.makedirs(FEATSDIR)\n",
    "add_features_and_create_new_dicts(datpaths, imgpaths, updatpaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 minutes\n",
    "gns = StandardScaler()\n",
    "for featpath in tqdm(updatpaths):\n",
    "    try:\n",
    "        celldatadict = joblib.load(featpath)\n",
    "        cellsfeats = np.array([v['intensity_feats'] for k, v in celldatadict.items()])\n",
    "        gns.partial_fit(cellsfeats)\n",
    "    except:\n",
    "        print(featpath)\n",
    "\n",
    "np.savez(FEATSCALERPATH, mean=gns.mean_, var=gns.var_)\n",
    "\n",
    "# dd = np.load(FEATSCALERPATH)\n",
    "# print(dd['mean'], gns.mean_)\n",
    "# print(dd['var'], gns.var_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATSCALERPATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare graphs : 4 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 1, 1]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.graph_construct import get_pids_multilabels_for_key\n",
    "\n",
    "df = pd.read_csv(CLINPATH)\n",
    "df =df.fillna(0)\n",
    "# display(df.info())\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "from os import path as osp\n",
    "import json\n",
    "import cv2\n",
    "from scipy.stats import skew\n",
    "import colorsys\n",
    "import random\n",
    "from typing import Dict, List, Tuple, Union\n",
    "\n",
    "from tiatoolbox.tools.graph import delaunay_adjacency, affinity_to_edge_index\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "SKEW_NOISE  = 0.0001\n",
    "\n",
    "\n",
    "def get_pids_labels_for_key(orig_df, key='OS', nclasses=3, idkey='patient_id'):\n",
    "    ckey = key+'_class'\n",
    "    df = orig_df[[idkey, key]]\n",
    "    \n",
    "    df[ckey] = int(-1)\n",
    "\n",
    "    separators = np.linspace(0, 1, nclasses+1)[0:-1]\n",
    "    for isep, sep in enumerate(separators):\n",
    "        sepval = df[key].quantile(sep)\n",
    "        sepmask = df[key] > sepval\n",
    "        df.loc[sepmask, ckey] = isep\n",
    "\n",
    "    df['Follow-up Status'] = orig_df['Follow-up Status']\n",
    "    df['OS'] = orig_df['OS']\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_pids_multilabels_for_key(orig_df, key_list, nclasses=3, idkey='patient_id'):\n",
    "    df = orig_df[[idkey]+ key_list]\n",
    "\n",
    "    for key in key_list:\n",
    "        if key == \"OS\":\n",
    "            ckey = key+'_class'\n",
    "            df[ckey] = int(0)\n",
    "            separators = np.linspace(0, 1, nclasses+1)[0:-1]\n",
    "            for isep, sep in enumerate(separators):\n",
    "                sepval = df[key].quantile(sep)\n",
    "                sepmask = df[key] > sepval\n",
    "                df.loc[sepmask, ckey] = isep\n",
    "        elif key in ['MYC IHC', 'BCL2 IHC', 'BCL6 IHC']:\n",
    "            ckey = key+'_class'\n",
    "            df[ckey] = int(0)\n",
    "            df[ckey][df[key] >= 40] = 1\n",
    "            df[ckey][df[key] < 40] = 0\n",
    "        else:\n",
    "            ckey = key+'_class'\n",
    "            df[ckey] = df[key]\n",
    "    df['OS'] = orig_df['OS']\n",
    "    return df\n",
    "\n",
    "if LABEL_TYPE == 'multilabel':\n",
    "    df_labels = get_pids_multilabels_for_key(df, key_list=['OS','MYC IHC', 'BCL2 IHC', 'BCL6 IHC', 'CD10 IHC', 'MUM1 IHC', 'Follow-up Status'], nclasses=2)\n",
    "else:\n",
    "    df_labels = get_pids_labels_for_key(df, key ='OS', nclasses=2)\n",
    "    # survival_event_data = df[['Follow-up Status']].to_numpy().tolist()\n",
    "    # survival_time_data = df[['OS']].to_numpy().tolist()\n",
    "\n",
    "\n",
    "len(df_labels)\n",
    "\n",
    "# Graph construction\n",
    "PATCH_SIZE = 300\n",
    "SKEW_NOISE = 0.0001\n",
    "MIN_CELLS_PER_PATCH = 10\n",
    "CONNECTIVITY_DISTANCE = 500\n",
    "\n",
    "# # save paths\n",
    "# featpaths = np.sort(glob.glob(f'{FEATSDIR}/**/*.dat', recursive=True)) #np.sort(glob.glob(f'{FEATSDIR}/*.dat'))\n",
    "# featpaths = [x if \"/0.dat\" not in x for x in featpaths]\n",
    "featpaths = np.sort(glob.glob(f'{FEATSDIR}/**/*.dat', recursive=True)) #np.sort(glob.glob(f'{FEATSDIR}/*.dat'))\n",
    "featpaths = [x for x in featpaths if (\"/0.dat\" not in x) and (\"/file_map.dat\" not in x)]\n",
    "featpaths\n",
    "pids = [int(osp.basename(featpath).split('_')[0]) for featpath in featpaths]\n",
    "df_featpaths = pd.DataFrame(zip(pids, featpaths), columns=['patient_id', 'featpath'])\n",
    "\n",
    "# merge to find datapoints with graph data and labels\n",
    "df_data = df_featpaths.merge(df_labels, on='patient_id')\n",
    "# df_data = df_data[:12]\n",
    "\n",
    "featpaths_data = df_data['featpath'].to_list()\n",
    "# labels_data = df_data['OS_class'].to_list()\n",
    "\n",
    "\n",
    "if LABEL_TYPE == 'multilabel':\n",
    "    labels_data = df_data[['OS_class','MYC IHC_class', 'BCL2 IHC_class', 'BCL6 IHC_class', 'CD10 IHC_class', 'MUM1 IHC_class']].to_numpy().tolist()\n",
    "    survival_event_data = df_data[['Follow-up Status_class']].to_numpy().tolist()\n",
    "    survival_time_data = df_data[['OS']].to_numpy().tolist()\n",
    "\n",
    "else:\n",
    "    labels_data = df_data['OS_class'].to_list()\n",
    "    survival_event_data = df_data[['Follow-up Status']].to_numpy().tolist()\n",
    "    survival_time_data = df_data[['OS']].to_numpy().tolist()\n",
    "\n",
    "\n",
    "display(labels_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((319, 5), 319)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.shape, len(survival_time_data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "from os import path as osp\n",
    "import json\n",
    "import cv2\n",
    "from scipy.stats import skew\n",
    "import colorsys\n",
    "import random\n",
    "from typing import Dict, List, Tuple, Union\n",
    "\n",
    "from tiatoolbox.tools.graph import delaunay_adjacency, affinity_to_edge_index\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "SKEW_NOISE  = 0.0001\n",
    "\n",
    "\n",
    "def get_pids_multilabels_for_key(df, key_list, nclasses=3, idkey='patient_id'):\n",
    "    df = df[[idkey]+ key_list]\n",
    "\n",
    "    for key in key_list:\n",
    "        ckey = key+'_class'\n",
    "        df[ckey] = int(0)\n",
    "        separators = np.linspace(0, 1, nclasses+1)[0:-1]\n",
    "        for isep, sep in enumerate(separators):\n",
    "            sepval = df[key].quantile(sep)\n",
    "            sepmask = df[key] > sepval\n",
    "            df.loc[sepmask, ckey] = isep\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_overall_statistics(features):\n",
    "    overall_mean = np.mean(features, axis=0).tolist()\n",
    "    overall_std = np.std(features, axis=0).tolist()\n",
    "    overall_var = np.var(features, axis=0).tolist()\n",
    "    overall_skewness = skew(features + np.random.randn(*features.shape)*SKEW_NOISE, axis=0).tolist()\n",
    "\n",
    "    # Calculate quantiles at percentiles 0.1, 0.2, ..., 0.9\n",
    "    quantiles = []\n",
    "    for q in [10,25,75,90]: #range(0, 100, 10):\n",
    "        quantiles += np.percentile(features, q, axis=0).tolist()\n",
    "\n",
    "    result_list = overall_mean + overall_std + overall_var + overall_skewness + quantiles\n",
    "    return result_list\n",
    "\n",
    "global_patch_stats = []\n",
    "\n",
    "def get_patch_pooled_positions_features(celldatadict, patch_size, cell_feat_norm_stats, MIN_CELLS_PER_PATCH):\n",
    "\n",
    "    global global_patch_stats\n",
    "\n",
    "    _cellfeats_gmean, _cellfeats_gvar = cell_feat_norm_stats\n",
    "    cf_gmean = np.expand_dims(_cellfeats_gmean, axis=0)\n",
    "    cf_gstd = np.expand_dims(np.sqrt(_cellfeats_gvar), axis=0)\n",
    "\n",
    "    cellids = [cellid for cellid, _ in celldatadict.items()]\n",
    "    cellposs = np.array([fdict['centroid'] for _, fdict in celldatadict.items()])\n",
    "    gridsize = (cellposs.max(axis=0) // patch_size + 1).astype(int).tolist()\n",
    "\n",
    "    # create list of patches\n",
    "    patches_grid_cids = [[[] for _ in range(gridsize[1])] for _ in range(gridsize[0])]\n",
    "\n",
    "    # sort cells into patches\n",
    "    for cellid, cellpos in zip(cellids, cellposs):\n",
    "        patch_coor = (cellpos // patch_size).astype(int).tolist()\n",
    "        patches_grid_cids[patch_coor[0]][patch_coor[1]].append(cellid)\n",
    "\n",
    "    patches_list_position = []\n",
    "    patches_list_features = []\n",
    "\n",
    "    # calc patch wise stats and add global list\n",
    "    for i, patches_row_cids in enumerate(patches_grid_cids):\n",
    "        for j, patch_cids in enumerate(patches_row_cids):\n",
    "            # if patch has less than eq 1 cell, skip creating it \n",
    "            if len(patch_cids) <= MIN_CELLS_PER_PATCH:\n",
    "                continue\n",
    "\n",
    "            global_patch_stats.append(len(patch_cids))\n",
    "\n",
    "            patch_cellposs = np.array([celldatadict[cellid]['centroid'] for cellid in patch_cids])\n",
    "            patch_cellfeats_raw = np.array([celldatadict[cellid]['intensity_feats'] for cellid in patch_cids])\n",
    "            patch_cellfeats = (patch_cellfeats_raw - cf_gmean) / cf_gstd\n",
    "\n",
    "            patch_position = np.mean(patch_cellposs, axis=0)\n",
    "            patch_features = np.array(get_overall_statistics(patch_cellfeats))\n",
    "\n",
    "            patches_list_position.append(patch_position)\n",
    "            patches_list_features.append(patch_features)\n",
    "\n",
    "    return np.array(patches_list_position), np.array(patches_list_features)\n",
    "\n",
    "def simple_delaunay(point_centroids, feature_centroids, connectivity_distance=4000):\n",
    "    adjacency_matrix = delaunay_adjacency(\n",
    "        points=point_centroids,\n",
    "        dthresh=connectivity_distance,\n",
    "    )\n",
    "    edge_index = affinity_to_edge_index(adjacency_matrix)\n",
    "    return {\n",
    "        \"x\": feature_centroids,\n",
    "        \"edge_index\": edge_index.astype(np.int64),\n",
    "        \"coordinates\": point_centroids,\n",
    "    }\n",
    "\n",
    "def create_graph_with_pooled_patch_nodes(featpaths, labels, outgraphpaths, patch_size, cell_feat_norm_stats , MIN_CELLS_PER_PATCH, CONNECTIVITY_DISTANCE):\n",
    "\n",
    "    def process_per_file_group(idx):\n",
    "        featpath = featpaths[idx]\n",
    "        label = labels[idx]\n",
    "        outgraphpath = outgraphpaths[idx]\n",
    "\n",
    "        celldatadict = joblib.load(featpath)\n",
    "        positions, features = get_patch_pooled_positions_features(celldatadict, patch_size, cell_feat_norm_stats,MIN_CELLS_PER_PATCH)\n",
    "\n",
    "        # graph cannot be constructed with only four patches\n",
    "        try:\n",
    "            graph_dict = simple_delaunay(\n",
    "                positions[:, :2],\n",
    "                features,\n",
    "                connectivity_distance=CONNECTIVITY_DISTANCE,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print('Skipping', featpath, 'due to', e)\n",
    "        else:\n",
    "            # Write a graph to a JSON file\n",
    "            with open(outgraphpath, 'w+') as handle:\n",
    "                # print(outgraphpath)\n",
    "                graph_dict = {k: v.tolist() for k, v in graph_dict.items()}\n",
    "                graph_dict['y'] = label\n",
    "                json.dump(graph_dict, handle)\n",
    "\n",
    "    joblib.Parallel(4)(\n",
    "        joblib.delayed(process_per_file_group)(fidx)\n",
    "        for fidx in tqdm(range(len(featpaths)), disable=False)\n",
    "    )\n",
    "    # for fidx in tqdm(range(len(featpaths)), disable=False):\n",
    "    #     process_per_file_group(fidx)\n",
    "\n",
    "\n",
    "def create_graph_with_pooled_patch_nodes_with_survival_data(featpaths, labels, survival_events, survival_time, outgraphpaths, patch_size, cell_feat_norm_stats , MIN_CELLS_PER_PATCH, CONNECTIVITY_DISTANCE):\n",
    "\n",
    "    def process_per_file_group(idx):\n",
    "        featpath = featpaths[idx]\n",
    "        label = labels[idx]\n",
    "        surv_event =survival_events[idx]\n",
    "        surv_time =survival_time[idx]\n",
    "\n",
    "        outgraphpath = outgraphpaths[idx]\n",
    "\n",
    "        celldatadict = joblib.load(featpath)\n",
    "        positions, features = get_patch_pooled_positions_features(celldatadict, patch_size, cell_feat_norm_stats,MIN_CELLS_PER_PATCH)\n",
    "\n",
    "        # graph cannot be constructed with only four patches\n",
    "        try:\n",
    "            graph_dict = simple_delaunay(\n",
    "                positions[:, :2],\n",
    "                features,\n",
    "                connectivity_distance=CONNECTIVITY_DISTANCE,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print('Skipping', featpath, 'due to', e)\n",
    "        else:\n",
    "            # Write a graph to a JSON file\n",
    "            with open(outgraphpath, 'w+') as handle:\n",
    "                # print(outgraphpath)\n",
    "                graph_dict = {k: v.tolist() for k, v in graph_dict.items()}\n",
    "                graph_dict['y'] = label\n",
    "                graph_dict['surv_event'] = surv_event\n",
    "                graph_dict['surv_time'] = surv_time\n",
    "\n",
    "                json.dump(graph_dict, handle)\n",
    "\n",
    "    joblib.Parallel(4)(\n",
    "        joblib.delayed(process_per_file_group)(fidx)\n",
    "        for fidx in tqdm(range(len(featpaths)), disable=False)\n",
    "    )\n",
    "    # for fidx in tqdm(range(len(featpaths)), disable=False):\n",
    "    #     process_per_file_group(fidx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/319 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 4/319 [00:00<00:10, 30.05it/s]|2023-12-11|21:43:59.271| [WARNING] /home/amrit/anaconda3/envs/DL2/lib/python3.10/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "\n",
      "|2023-12-11|21:43:59.271| [WARNING] /home/amrit/anaconda3/envs/DL2/lib/python3.10/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "\n",
      "|2023-12-11|21:43:59.271| [WARNING] /home/amrit/anaconda3/envs/DL2/lib/python3.10/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "\n",
      "|2023-12-11|21:43:59.299| [WARNING] /home/amrit/anaconda3/envs/DL2/lib/python3.10/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "\n",
      "|2023-12-11|21:43:59.299| [WARNING] /home/amrit/anaconda3/envs/DL2/lib/python3.10/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "\n",
      "|2023-12-11|21:43:59.299| [WARNING] /home/amrit/anaconda3/envs/DL2/lib/python3.10/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "\n",
      "|2023-12-11|21:43:59.331| [WARNING] /home/amrit/anaconda3/envs/DL2/lib/python3.10/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "\n",
      "|2023-12-11|21:43:59.331| [WARNING] /home/amrit/anaconda3/envs/DL2/lib/python3.10/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "\n",
      "|2023-12-11|21:43:59.331| [WARNING] /home/amrit/anaconda3/envs/DL2/lib/python3.10/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "\n",
      "|2023-12-11|21:43:59.376| [WARNING] /home/amrit/anaconda3/envs/DL2/lib/python3.10/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "\n",
      "|2023-12-11|21:43:59.376| [WARNING] /home/amrit/anaconda3/envs/DL2/lib/python3.10/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "\n",
      "|2023-12-11|21:43:59.377| [WARNING] /home/amrit/anaconda3/envs/DL2/lib/python3.10/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "\n",
      "|2023-12-11|21:44:03.187| [WARNING] /home/amrit/anaconda3/envs/DL2/lib/python3.10/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "\n",
      "|2023-12-11|21:44:03.249| [WARNING] /home/amrit/anaconda3/envs/DL2/lib/python3.10/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "\n",
      "|2023-12-11|21:44:03.265| [WARNING] /home/amrit/anaconda3/envs/DL2/lib/python3.10/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "\n",
      "|2023-12-11|21:44:03.407| [WARNING] /home/amrit/anaconda3/envs/DL2/lib/python3.10/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|2023-12-11|21:44:03.616| [INFO] Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "|2023-12-11|21:44:03.683| [INFO] Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "|2023-12-11|21:44:03.696| [INFO] Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "|2023-12-11|21:44:03.840| [INFO] Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 319/319 [01:19<00:00,  4.03it/s]\n"
     ]
    }
   ],
   "source": [
    "outgraphpaths_data = [f\"{GRAPHSDIR}/{osp.basename(featpath).split('.')[0]}.json\" for featpath in featpaths_data]\n",
    "\n",
    "# save labels\n",
    "labels_dict = {osp.basename(graphpath): label for graphpath, label in zip(outgraphpaths_data, labels_data)}\n",
    "with open(LABELSPATH, 'w') as f:\n",
    "    json.dump(labels_dict, f)\n",
    "\n",
    "# read normalizer stats from file and pass to fn\n",
    "dd = np.load(FEATSCALERPATH)\n",
    "cell_feat_norm_stats = (dd['mean'], dd['var'])\n",
    "\n",
    "# create final graphs data\n",
    "shutil.rmtree(GRAPHSDIR)\n",
    "if not osp.exists(GRAPHSDIR):\n",
    "    os.makedirs(GRAPHSDIR)\n",
    "    create_graph_with_pooled_patch_nodes_with_survival_data(\n",
    "        featpaths_data,\n",
    "        labels_data,\n",
    "        survival_event_data,\n",
    "        survival_time_data,\n",
    "        outgraphpaths_data,\n",
    "        PATCH_SIZE,\n",
    "        cell_feat_norm_stats=cell_feat_norm_stats,\n",
    "        MIN_CELLS_PER_PATCH= MIN_CELLS_PER_PATCH,\n",
    "        CONNECTIVITY_DISTANCE = CONNECTIVITY_DISTANCE\n",
    "    )\n",
    "    # wont work in parallel mode\n",
    "    # print(np.mean(global_patch_stats), np.std(global_patch_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outgraphpaths_data = [f\"{GRAPHSDIR}/{osp.basename(featpath).split('.')[0]}.json\" for featpath in featpaths_data]\n",
    "\n",
    "# save labels\n",
    "labels_dict = {osp.basename(graphpath): label for graphpath, label in zip(outgraphpaths_data, labels_data)}\n",
    "with open(LABELSPATH, 'w') as f:\n",
    "    json.dump(labels_dict, f)\n",
    "\n",
    "# read normalizer stats from file and pass to fn\n",
    "dd = np.load(FEATSCALERPATH)\n",
    "cell_feat_norm_stats = (dd['mean'], dd['var'])\n",
    "\n",
    "# create final graphs data\n",
    "shutil.rmtree(GRAPHSDIR)\n",
    "if not osp.exists(GRAPHSDIR):\n",
    "    os.makedirs(GRAPHSDIR)\n",
    "    create_graph_with_pooled_patch_nodes(\n",
    "        featpaths_data,\n",
    "        labels_data,\n",
    "        outgraphpaths_data,\n",
    "        PATCH_SIZE,\n",
    "        cell_feat_norm_stats=cell_feat_norm_stats,\n",
    "        MIN_CELLS_PER_PATCH= MIN_CELLS_PER_PATCH,\n",
    "        CONNECTIVITY_DISTANCE = CONNECTIVITY_DISTANCE\n",
    "    )\n",
    "    # wont work in parallel mode\n",
    "    # print(np.mean(global_patch_stats), np.std(global_patch_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(319,\n",
       " 319,\n",
       " ['13901_MYC_532',\n",
       "  '13901_MYC_557',\n",
       "  '13902_MYC_513',\n",
       "  '13902_MYC_514',\n",
       "  '13903_MYC_535'])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wsi_paths = recur_find_ext(GRAPHSDIR, [\".json\"])\n",
    "wsi_names = [Path(v).stem for v in wsi_paths]\n",
    "assert len(wsi_paths) > 0, \"No files found.\"  # noqa: S101\n",
    "\n",
    "len(wsi_paths) , len(wsi_names) , wsi_names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Callable\n",
    "import ujson as json\n",
    "import numpy as np\n",
    "import pathlib\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Sampler\n",
    "from torch_geometric.data import Data, Dataset\n",
    "\n",
    "\n",
    "class SlideGraphDataset(Dataset):\n",
    "    \"\"\"Handling loading graph data from disk.\n",
    "\n",
    "    Args:\n",
    "        info_list (list): In case of `train` or `valid` is in `mode`,\n",
    "            this is expected to be a list of `[uid, label]` . Otherwise,\n",
    "            it is a list of `uid`. Here, `uid` is used to construct\n",
    "            `f\"{GRAPH_DIR}/{wsi_code}.json\"` which is a path points to\n",
    "            a `.json` file containing the graph structure. By `label`, we mean\n",
    "            the label of the graph. The format within the `.json` file comes\n",
    "            from `tiatoolbox.tools.graph`.\n",
    "        mode (str): This denotes which data mode the `info_list` is in.\n",
    "        preproc (callable): The prerocessing function for each node\n",
    "            within the graph.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self: Dataset,\n",
    "        info_list: list,\n",
    "        mode: str = \"train\",\n",
    "        graph_dir: pathlib.Path = None,\n",
    "        preproc: Callable | None = None,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize SlideGraphDataset.\"\"\"\n",
    "        self.info_list = info_list\n",
    "        self.mode = mode\n",
    "        self.graph_dir = graph_dir\n",
    "        self.preproc = preproc\n",
    "\n",
    "    def __getitem__(self: Dataset, idx: int) -> Dataset:\n",
    "        \"\"\"Get an element from SlideGraphDataset.\"\"\"\n",
    "        info = self.info_list[idx]\n",
    "        if any(v in self.mode for v in [\"train\", \"valid\"]):\n",
    "            wsi_code, label = info\n",
    "            # torch.Tensor will create 1-d vector not scalar\n",
    "            label = torch.tensor(label)\n",
    "        else:\n",
    "            wsi_code = info\n",
    "\n",
    "        with (self.graph_dir / f\"{wsi_code}.json\").open() as fptr:\n",
    "            graph_dict = json.load(fptr)\n",
    "        graph_dict = {k: np.array(v) for k, v in graph_dict.items()}\n",
    "\n",
    "        if self.preproc is not None:\n",
    "            graph_dict[\"x\"] = self.preproc(graph_dict[\"x\"])\n",
    "\n",
    "        graph_dict = {k: torch.tensor(v) for k, v in graph_dict.items()}\n",
    "        graph = Data(**graph_dict)\n",
    "\n",
    "        if any(v in self.mode for v in [\"train\", \"valid\"]):\n",
    "            return {\"graph\": graph, \"label\": label}\n",
    "        return {\"graph\": graph}\n",
    "\n",
    "    def __len__(self: Dataset) -> int:\n",
    "        \"\"\"Length of SlideGraphDataset.\"\"\"\n",
    "        return len(self.info_list)\n",
    "\n",
    "    def len(self):\n",
    "        return self.__len__()\n",
    "\n",
    "    def get(self, idx):\n",
    "        return self.__getitem__(idx)\n",
    "\n",
    "\n",
    "# def stratified_split(\n",
    "#     x: list,\n",
    "#     y: list,\n",
    "#     train: float,\n",
    "#     valid: float,\n",
    "#     test: float,\n",
    "#     num_folds: int,\n",
    "#     seed: int = 5,\n",
    "# ) -> list:\n",
    "#     \"\"\"Helper to generate stratified splits.\n",
    "\n",
    "#     Split `x` and `y` in to N number of `num_folds` sets\n",
    "#     of `train`, `valid`, and `test` set in stratified manner.\n",
    "#     `train`, `valid`, and `test` are guaranteed to be mutually\n",
    "#     exclusive.\n",
    "\n",
    "#     Args:\n",
    "#         x (list, np.ndarray):\n",
    "#             List of samples.\n",
    "#         y (list, np.ndarray):\n",
    "#             List of labels, each value is the value\n",
    "#             of the sample at the same index in `x`.\n",
    "#         train (float):\n",
    "#             Percentage to be used for training set.\n",
    "#         valid (float):\n",
    "#             Percentage to be used for validation set.\n",
    "#         test (float):\n",
    "#             Percentage to be used for testing set.\n",
    "#         num_folds (int):\n",
    "#             Number of split generated.\n",
    "#         seed (int):\n",
    "#             Random seed. Default=5.\n",
    "\n",
    "#     Returns:\n",
    "#         A list of splits where each is a dictionary of\n",
    "#         {\n",
    "#             'train': [(sample_A, label_A), (sample_B, label_B), ...],\n",
    "#             'valid': [(sample_C, label_C), (sample_D, label_D), ...],\n",
    "#             'test' : [(sample_E, label_E), (sample_E, label_E), ...],\n",
    "#         }\n",
    "\n",
    "#     \"\"\"\n",
    "#     assert (  # noqa: S101\n",
    "#         train + valid + test - 1.0 < 1.0e-10  # noqa: PLR2004\n",
    "#     ), \"Ratios must sum to 1.0 .\"\n",
    "\n",
    "#     outer_splitter = StratifiedShuffleSplit(\n",
    "#         n_splits=num_folds,\n",
    "#         train_size=train + valid,\n",
    "#         random_state=seed,\n",
    "#     )\n",
    "#     inner_splitter = StratifiedShuffleSplit(\n",
    "#         n_splits=1,\n",
    "#         train_size=train / (train + valid),\n",
    "#         random_state=seed,\n",
    "#     )\n",
    "\n",
    "#     x = np.array(x)\n",
    "#     y = np.array(y)\n",
    "#     splits = []\n",
    "#     for train_valid_idx, test_idx in outer_splitter.split(x, y):\n",
    "#         test_x = x[test_idx]\n",
    "#         test_y = y[test_idx]\n",
    "\n",
    "#         # Holder for train_valid set\n",
    "#         x_ = x[train_valid_idx]\n",
    "#         y_ = y[train_valid_idx]\n",
    "\n",
    "#         # Split train_valid into train and valid set\n",
    "#         train_idx, valid_idx = next(iter(inner_splitter.split(x_, y_)))\n",
    "#         valid_x = x_[valid_idx]\n",
    "#         valid_y = y_[valid_idx]\n",
    "\n",
    "#         train_x = x_[train_idx]\n",
    "#         train_y = y_[train_idx]\n",
    "\n",
    "#         # Integrity check\n",
    "#         assert len(set(train_x).intersection(set(valid_x))) == 0  # noqa: S101\n",
    "#         assert len(set(valid_x).intersection(set(test_x))) == 0  # noqa: S101\n",
    "#         assert len(set(train_x).intersection(set(test_x))) == 0  # noqa: S101\n",
    "\n",
    "#         splits.append(\n",
    "#             {\n",
    "#                 \"train\": list(zip(train_x, train_y)),\n",
    "#                 \"valid\": list(zip(valid_x, valid_y)),\n",
    "#                 \"test\": list(zip(test_x, test_y)),\n",
    "#             },\n",
    "#         )\n",
    "#     return splits\n",
    "\n",
    "\n",
    "\n",
    "def stratified_split(\n",
    "    x: list,\n",
    "    y: list,\n",
    "    train: float,\n",
    "    valid: float,\n",
    "    test: float,\n",
    "    num_folds: int,\n",
    "    seed: int = 5,\n",
    ") -> list:\n",
    "    \"\"\"Helper to generate stratified splits.\n",
    "\n",
    "    Split `x` and `y` in to N number of `num_folds` sets\n",
    "    of `train`, `valid`, and `test` set in stratified manner.\n",
    "    `train`, `valid`, and `test` are guaranteed to be mutually\n",
    "    exclusive.\n",
    "\n",
    "    Args:\n",
    "        x (list, np.ndarray):\n",
    "            List of samples.\n",
    "        y (list, np.ndarray):\n",
    "            List of labels, each value is the value\n",
    "            of the sample at the same index in `x`.\n",
    "        train (float):\n",
    "            Percentage to be used for training set.\n",
    "        valid (float):\n",
    "            Percentage to be used for validation set.\n",
    "        test (float):\n",
    "            Percentage to be used for testing set.\n",
    "        num_folds (int):\n",
    "            Number of split generated.\n",
    "        seed (int):\n",
    "            Random seed. Default=5.\n",
    "\n",
    "    Returns:\n",
    "        A list of splits where each is a dictionary of\n",
    "        {\n",
    "            'train': [(sample_A, label_A), (sample_B, label_B), ...],\n",
    "            'valid': [(sample_C, label_C), (sample_D, label_D), ...],\n",
    "            'test' : [(sample_E, label_E), (sample_E, label_E), ...],\n",
    "        }\n",
    "\n",
    "    \"\"\"\n",
    "    assert (  # noqa: S101\n",
    "        train + valid + test - 1.0 < 1.0e-10  # noqa: PLR2004\n",
    "    ), \"Ratios must sum to 1.0 .\"\n",
    "\n",
    "    outer_splitter = StratifiedShuffleSplit(\n",
    "        n_splits=num_folds,\n",
    "        train_size=train + valid,\n",
    "        random_state=seed,\n",
    "    )\n",
    "    inner_splitter = StratifiedShuffleSplit(\n",
    "        n_splits=1,\n",
    "        train_size=train / (train + valid),\n",
    "        random_state=seed,\n",
    "    )\n",
    "\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "\n",
    "    print(x,y)\n",
    "    splits = []\n",
    "    for train_valid_idx, test_idx in outer_splitter.split(x, y):\n",
    "        test_x = x[test_idx]\n",
    "        test_y = y[test_idx]\n",
    "\n",
    "        # Holder for train_valid set\n",
    "        x_ = x[train_valid_idx]\n",
    "        y_ = y[train_valid_idx]\n",
    "\n",
    "        # Split train_valid into train and valid set\n",
    "        train_idx, valid_idx = next(iter(inner_splitter.split(x_, y_)))\n",
    "        valid_x = x_[valid_idx]\n",
    "        valid_y = y_[valid_idx]\n",
    "\n",
    "        train_x = x_[train_idx]\n",
    "        train_y = y_[train_idx]\n",
    "\n",
    "        # Integrity check\n",
    "        assert len(set(train_x).intersection(set(valid_x))) == 0  # noqa: S101\n",
    "        assert len(set(valid_x).intersection(set(test_x))) == 0  # noqa: S101\n",
    "        assert len(set(train_x).intersection(set(test_x))) == 0  # noqa: S101\n",
    "\n",
    "        splits.append(\n",
    "            {\n",
    "                \"train\": list(zip(train_x, train_y)),\n",
    "                \"valid\": list(zip(valid_x, valid_y)),\n",
    "                \"test\": list(zip(test_x, test_y)),\n",
    "            },\n",
    "        )\n",
    "    return splits\n",
    "\n",
    "\n",
    "def multilabel_stratified_split(\n",
    "    x: list,\n",
    "    y: list,\n",
    "    train: float,\n",
    "    valid: float,\n",
    "    test: float,\n",
    "    num_folds: int,\n",
    "    seed: int = 5,\n",
    ") -> list:\n",
    "    \"\"\"Helper to generate stratified splits.\n",
    "\n",
    "    Split `x` and `y` in to N number of `num_folds` sets\n",
    "    of `train`, `valid`, and `test` set in stratified manner.\n",
    "    `train`, `valid`, and `test` are guaranteed to be mutually\n",
    "    exclusive.\n",
    "\n",
    "    Args:\n",
    "        x (list, np.ndarray):\n",
    "            List of samples.\n",
    "        y (list, np.ndarray):\n",
    "            List of labels, each value is the value\n",
    "            of the sample at the same index in `x`.\n",
    "        train (float):\n",
    "            Percentage to be used for training set.\n",
    "        valid (float):\n",
    "            Percentage to be used for validation set.\n",
    "        test (float):\n",
    "            Percentage to be used for testing set.\n",
    "        num_folds (int):\n",
    "            Number of split generated.\n",
    "        seed (int):\n",
    "            Random seed. Default=5.\n",
    "\n",
    "    Returns:\n",
    "        A list of splits where each is a dictionary of\n",
    "        {\n",
    "            'train': [(sample_A, label_A), (sample_B, label_B), ...],\n",
    "            'valid': [(sample_C, label_C), (sample_D, label_D), ...],\n",
    "            'test' : [(sample_E, label_E), (sample_E, label_E), ...],\n",
    "        }\n",
    "\n",
    "    \"\"\"\n",
    "    assert (  # noqa: S101\n",
    "        train + valid + test - 1.0 < 1.0e-10  # noqa: PLR2004\n",
    "    ), \"Ratios must sum to 1.0 .\"\n",
    "\n",
    "    outer_splitter = StratifiedShuffleSplit(\n",
    "        n_splits=num_folds,\n",
    "        train_size=train + valid,\n",
    "        random_state=seed,\n",
    "    )\n",
    "    inner_splitter = StratifiedShuffleSplit(\n",
    "        n_splits=1,\n",
    "        train_size=train / (train + valid),\n",
    "        random_state=seed,\n",
    "    )\n",
    "\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    splits = []\n",
    "    for train_valid_idx, test_idx in outer_splitter.split(x, y[:,0]):\n",
    "        test_x = x[test_idx]\n",
    "        test_y = y[test_idx]\n",
    "\n",
    "        # Holder for train_valid set\n",
    "        x_ = x[train_valid_idx]\n",
    "        y_ = y[train_valid_idx]\n",
    "\n",
    "        # Split train_valid into train and valid set\n",
    "        train_idx, valid_idx = next(iter(inner_splitter.split(x_, y_[:,0])))\n",
    "        valid_x = x_[valid_idx]\n",
    "        valid_y = y_[valid_idx]\n",
    "\n",
    "        train_x = x_[train_idx]\n",
    "        train_y = y_[train_idx]\n",
    "\n",
    "        # Integrity check\n",
    "        assert len(set(train_x).intersection(set(valid_x))) == 0  # noqa: S101\n",
    "        assert len(set(valid_x).intersection(set(test_x))) == 0  # noqa: S101\n",
    "        assert len(set(train_x).intersection(set(test_x))) == 0  # noqa: S101\n",
    "\n",
    "        splits.append(\n",
    "            {\n",
    "                \"train\": list(zip(train_x, train_y)),\n",
    "                \"valid\": list(zip(valid_x, valid_y)),\n",
    "                \"test\": list(zip(test_x, test_y)),\n",
    "            },\n",
    "        )\n",
    "    return splits\n",
    "\n",
    "\n",
    "class StratifiedSampler(Sampler):\n",
    "    \"\"\"Sampling the dataset such that the batch contains stratified samples.\n",
    "\n",
    "    Args:\n",
    "        labels (list): List of labels, must be in the same ordering as input\n",
    "            samples provided to the `SlideGraphDataset` object.\n",
    "        batch_size (int): Size of the batch.\n",
    "\n",
    "    Returns:\n",
    "        List of indices to query from the `SlideGraphDataset` object.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self: Sampler, labels: list, batch_size: int = 10) -> None:\n",
    "        \"\"\"Initialize StratifiedSampler.\"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.num_splits = int(len(labels) / self.batch_size)\n",
    "        self.labels = labels\n",
    "        self.num_steps = self.num_splits\n",
    "\n",
    "    def _sampling(self: Sampler) -> list:\n",
    "        \"\"\"Do we want to control randomness here.\"\"\"\n",
    "        skf = StratifiedKFold(n_splits=self.num_splits, shuffle=True)\n",
    "        indices = np.arange(len(self.labels))  # idx holder\n",
    "        # return array of arrays of indices in each batch\n",
    "        return [tidx for _, tidx in skf.split(indices, self.labels)]\n",
    "\n",
    "    def __iter__(self: Sampler) -> Iterator:\n",
    "        \"\"\"Define Iterator.\"\"\"\n",
    "        return iter(self._sampling())\n",
    "\n",
    "    def __len__(self: Sampler) -> int:\n",
    "        \"\"\"The length of the sampler.\n",
    "\n",
    "        This value actually corresponds to the number of steps to query\n",
    "        sampled batch indices. Thus, to maintain epoch and steps hierarchy,\n",
    "        this should be equal to the number of expected steps as in usual\n",
    "        sampling: `steps=dataset_size / batch_size`.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.num_steps\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from torch.utils.data import Sampler\n",
    "\n",
    "\n",
    "\n",
    "class StratifiedSampler_multilabel(Sampler):\n",
    "    \"\"\"Sampling the dataset such that the batch contains stratified samples.\n",
    "\n",
    "    Args:\n",
    "        labels (list): List of labels, must be in the same ordering as input\n",
    "            samples provided to the `SlideGraphDataset` object.\n",
    "        batch_size (int): Size of the batch.\n",
    "\n",
    "    Returns:\n",
    "        List of indices to query from the `SlideGraphDataset` object.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, labels: list, batch_size: int = 10):\n",
    "        \"\"\"Initialize StratifiedSampler.\"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = np.array(labels)\n",
    "        self.sss = StratifiedShuffleSplit(n_splits=1, test_size=batch_size / len(labels), random_state=42)\n",
    "\n",
    "    def _sampling(self):\n",
    "        \"\"\"Do we want to control randomness here.\"\"\"\n",
    "        indices = np.arange(len(self.labels))\n",
    "        _, tidx = next(self.sss.split(indices, self.labels[:,0]))\n",
    "        # return array of arrays of indices in each batch\n",
    "        return [tidx]\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Define Iterator.\"\"\"\n",
    "        return iter(self._sampling())\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"The length of the sampler.\"\"\"\n",
    "        return len(self.labels) // self.batch_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'13901_MYC_532.json': 0, '13901_MYC_557.json': 0, '13902_MYC_513.json': 1, '13902_MYC_514.json': 1, '13903_MYC_535.json': 1, '13903_MYC_560.json': 1, '13904_MYC_539.json': 0, '13904_MYC_550.json': 0, '13908_MYC_527.json': 1, '13908_MYC_555.json': 1, '13911_MYC_520.json': 1, '13911_MYC_533.json': 1, '13912_MYC_530.json': 0, '13913_MYC_536.json': 0, '13913_MYC_554.json': 0, '13914_MYC_534.json': 0, '13914_MYC_556.json': 0, '13915_MYC_516.json': 1, '13915_MYC_547.json': 1, '13917_MYC_526.json': 1, '13917_MYC_551.json': 1, '13919_MYC_518.json': 0, '13919_MYC_528.json': 0, '13920_MYC_549.json': 1, '13922_MYC_558.json': 1, '13923_MYC_525.json': 1, '13923_MYC_538.json': 1, '13924_MYC_515.json': 1, '13924_MYC_548.json': 1, '13952_MYC_2587.json': 1, '13952_MYC_2589.json': 1, '13953_MYC_2611.json': 1, '13953_MYC_2639.json': 1, '13954_MYC_2592.json': 1, '13955_MYC_2637.json': 0, '13957_MYC_2646.json': 0, '13958_MYC_2600.json': 1, '13958_MYC_2640.json': 1, '13959_MYC_2593.json': 1, '13959_MYC_2644.json': 1, '13960_MYC_2597.json': 1, '13961_MYC_2613.json': 0, '13961_MYC_2626.json': 0, '13962_MYC_2585.json': 1, '13962_MYC_2605.json': 1, '13963_MYC_2610.json': 0, '13963_MYC_2635.json': 0, '13964_MYC_2632.json': 1, '13965_MYC_2604.json': 1, '13965_MYC_2606.json': 1, '13966_MYC_2603.json': 1, '13966_MYC_2617.json': 1, '13971_MYC_2596.json': 1, '13971_MYC_2634.json': 1, '13972_MYC_2623.json': 1, '13972_MYC_2636.json': 1, '13973_MYC_2619.json': 1, '13973_MYC_2641.json': 1, '13974_MYC_2588.json': 1, '13974_MYC_2633.json': 1, '13976_MYC_2586.json': 1, '13976_MYC_2620.json': 1, '13977_MYC_2599.json': 1, '13977_MYC_2629.json': 1, '13978_MYC_2608.json': 1, '13978_MYC_2621.json': 1, '13979_MYC_2618.json': 0, '13979_MYC_2631.json': 0, '13980_MYC_2591.json': 0, '13980_MYC_2622.json': 0, '13981_MYC_2609.json': 1, '13981_MYC_2615.json': 1, '13983_MYC_2594.json': 1, '13983_MYC_2627.json': 1, '13986_MYC_2607.json': 1, '13986_MYC_2625.json': 1, '13988_MYC_2630.json': 1, '13989_MYC_2614.json': 0, '17643_MYC_2555.json': 0, '17643_MYC_2583.json': 0, '17648_MYC_2534.json': 1, '17648_MYC_2570.json': 1, '17649_MYC_2535.json': 1, '17649_MYC_2536.json': 1, '17650_MYC_2546.json': 1, '17650_MYC_2569.json': 1, '17653_MYC_2581.json': 0, '17654_MYC_2564.json': 1, '17654_MYC_2584.json': 1, '17656_MYC_2560.json': 1, '17656_MYC_2567.json': 1, '17657_MYC_2541.json': 1, '17657_MYC_2551.json': 1, '17658_MYC_2554.json': 1, '17658_MYC_2563.json': 1, '17660_MYC_2540.json': 1, '17660_MYC_2575.json': 1, '17662_MYC_2559.json': 0, '17662_MYC_2562.json': 0, '17664_MYC_2552.json': 1, '17664_MYC_2579.json': 1, '17666_MYC_2543.json': 0, '17666_MYC_2548.json': 0, '17667_MYC_2532.json': 1, '17667_MYC_2537.json': 1, '17669_MYC_2574.json': 1, '17670_MYC_2539.json': 1, '17670_MYC_2572.json': 1, '17673_MYC_2557.json': 1, '17673_MYC_2571.json': 1, '18828_MYC_2347.json': 1, '18828_MYC_2352.json': 1, '18829_MYC_2350.json': 1, '18829_MYC_2354.json': 1, '18830_MYC_2341.json': 1, '18830_MYC_2358.json': 1, '18831_MYC_2316.json': 1, '18831_MYC_2344.json': 1, '18831_MYC_2364.json': 1, '18833_MYC_2355.json': 0, '18834_MYC_2336.json': 0, '18834_MYC_2346.json': 0, '18835_MYC_2361.json': 0, '18835_MYC_2367.json': 0, '18836_MYC_2315.json': 1, '18836_MYC_2351.json': 1, '18837_MYC_2327.json': 0, '18837_MYC_2373.json': 0, '18838_MYC_2342.json': 1, '18838_MYC_2366.json': 1, '18839_MYC_2339.json': 0, '18840_MYC_2319.json': 1, '18840_MYC_2356.json': 1, '18841_MYC_2343.json': 1, '18843_MYC_2330.json': 0, '18843_MYC_2370.json': 0, '18844_MYC_2357.json': 1, '18845_MYC_2332.json': 1, '18845_MYC_2353.json': 1, '18846_MYC_2320.json': 1, '18846_MYC_2338.json': 1, '18847_MYC_2321.json': 1, '18847_MYC_2362.json': 1, '18848_MYC_2349.json': 0, '18848_MYC_2374.json': 0, '18849_MYC_2314.json': 0, '18849_MYC_2337.json': 0, '18851_MYC_2363.json': 0, '18851_MYC_2372.json': 0, '18852_MYC_2325.json': 0, '18852_MYC_2334.json': 0, '19667_MYC_2265.json': 0, '19667_MYC_2276.json': 0, '19669_MYC_2252.json': 1, '19669_MYC_2286.json': 1, '19670_MYC_2291.json': 1, '19670_MYC_2293.json': 1, '19672_MYC_2254.json': 1, '19672_MYC_2304.json': 1, '19673_MYC_2259.json': 0, '19673_MYC_2269.json': 0, '19674_MYC_2264.json': 0, '19674_MYC_2303.json': 0, '19675_MYC_2277.json': 1, '19675_MYC_2309.json': 1, '19676_MYC_2262.json': 1, '19676_MYC_2308.json': 1, '19677_MYC_2255.json': 0, '19678_MYC_2287.json': 1, '19679_MYC_2275.json': 1, '19679_MYC_2290.json': 1, '19681_MYC_2257.json': 1, '19681_MYC_2310.json': 1, '19682_MYC_2296.json': 0, '19682_MYC_2301.json': 0, '19683_MYC_2253.json': 1, '19683_MYC_2271.json': 1, '19684_MYC_2272.json': 1, '19684_MYC_2285.json': 1, '19685_MYC_2280.json': 1, '19685_MYC_2289.json': 1, '19687_MYC_2292.json': 1, '19687_MYC_2297.json': 1, '19688_MYC_2288.json': 0, '19688_MYC_2313.json': 0, '19689_MYC_2298.json': 1, '19689_MYC_2299.json': 1, '19690_MYC_2273.json': 0, '19690_MYC_2294.json': 0, '19691_MYC_2282.json': 0, '19691_MYC_2284.json': 0, '19692_MYC_2279.json': 0, '19692_MYC_2281.json': 0, '19693_MYC_2260.json': 0, '19693_MYC_2278.json': 0, '19694_MYC_2283.json': 0, '19694_MYC_2302.json': 0, '19695_MYC_2270.json': 0, '19695_MYC_2295.json': 0, '19697_MYC_2256.json': 0, '19697_MYC_2274.json': 0, '19698_MYC_2300.json': 1, '19698_MYC_2306.json': 1, '19700_MYC_2258.json': 0, '19700_MYC_2268.json': 0, '26786_MYC_1916.json': 0, '26786_MYC_1961.json': 0, '26787_MYC_1941.json': 0, '26787_MYC_1944.json': 0, '26788_MYC_1925.json': 1, '26788_MYC_1937.json': 1, '26789_MYC_1934.json': 1, '26789_MYC_1949.json': 1, '26790_MYC_1939.json': 0, '26790_MYC_1969.json': 0, '26791_MYC_1907.json': 0, '26791_MYC_1951.json': 0, '26792_MYC_1918.json': 0, '26793_MYC_1922.json': 0, '26793_MYC_1968.json': 0, '26794_MYC_1909.json': 1, '26794_MYC_1936.json': 1, '26795_MYC_1928.json': 1, '26795_MYC_1959.json': 1, '26796_MYC_1912.json': 1, '26796_MYC_1931.json': 1, '26797_MYC_1914.json': 0, '26798_MYC_1908.json': 0, '26798_MYC_1947.json': 0, '26799_MYC_1943.json': 0, '26799_MYC_1963.json': 0, '26800_MYC_1970.json': 0, '26801_MYC_1921.json': 0, '26801_MYC_1957.json': 0, '26802_MYC_1940.json': 0, '26802_MYC_1948.json': 0, '26803_MYC_1924.json': 0, '26803_MYC_1932.json': 0, '26804_MYC_1929.json': 1, '26804_MYC_1971.json': 1, '26805_MYC_1913.json': 1, '26805_MYC_1923.json': 1, '26806_MYC_1966.json': 0, '26808_MYC_1935.json': 0, '26808_MYC_1942.json': 0, '26809_MYC_1933.json': 0, '26809_MYC_1956.json': 0, '26810_MYC_1926.json': 0, '26810_MYC_1964.json': 0, '26811_MYC_1952.json': 0, '26812_MYC_1930.json': 0, '26812_MYC_1960.json': 0, '26813_MYC_629.json': 0, '26814_MYC_599.json': 0, '26816_MYC_605.json': 0, '26816_MYC_610.json': 0, '26817_MYC_617.json': 0, '26817_MYC_631.json': 0, '26818_MYC_583.json': 0, '26819_MYC_575.json': 1, '26819_MYC_634.json': 1, '26820_MYC_611.json': 0, '26820_MYC_618.json': 0, '26821_MYC_600.json': 0, '26821_MYC_603.json': 0, '26822_MYC_566.json': 1, '26822_MYC_628.json': 1, '26823_MYC_602.json': 0, '26823_MYC_626.json': 0, '26824_MYC_604.json': 0, '26825_MYC_580.json': 0, '26825_MYC_633.json': 0, '26826_MYC_582.json': 0, '26828_MYC_607.json': 0, '26828_MYC_616.json': 0, '26829_MYC_581.json': 0, '26830_MYC_563.json': 1, '26830_MYC_585.json': 1, '26831_MYC_591.json': 1, '26832_MYC_615.json': 0, '26832_MYC_622.json': 0, '26833_MYC_564.json': 0, '26833_MYC_567.json': 0, '26834_MYC_587.json': 0, '26834_MYC_635.json': 0, '26835_MYC_592.json': 0, '26836_MYC_570.json': 0, '26838_MYC_574.json': 0, '26838_MYC_578.json': 0, '26839_MYC_593.json': 0, '26839_MYC_624.json': 0, '26841_MYC_595.json': 0, '26841_MYC_608.json': 0, '26842_MYC_573.json': 1, '26842_MYC_586.json': 1, '26843_MYC_612.json': 0, '26843_MYC_614.json': 0, '26844_MYC_609.json': 0, '26845_MYC_565.json': 0, '26845_MYC_625.json': 0, '26846_MYC_571.json': 0, '26846_MYC_623.json': 0, '26847_MYC_568.json': 0, '26847_MYC_630.json': 0, '26848_MYC_584.json': 0, '26848_MYC_594.json': 0, '26849_MYC_598.json': 0, '26849_MYC_619.json': 0, '26850_MYC_576.json': 0, '26850_MYC_620.json': 0, '26851_MYC_589.json': 1, '26851_MYC_596.json': 1, '26853_MYC_579.json': 0, '26853_MYC_597.json': 0, '26857_MYC_561.json': 0, '26858_MYC_601.json': 1, '26858_MYC_606.json': 1, '26859_MYC_572.json': 1, '26859_MYC_627.json': 1}\n",
      "{'13901_MYC_532.json': 0, '13901_MYC_557.json': 0, '13902_MYC_513.json': 1, '13902_MYC_514.json': 1, '13903_MYC_535.json': 1, '13903_MYC_560.json': 1, '13904_MYC_539.json': 0, '13904_MYC_550.json': 0, '13908_MYC_527.json': 1, '13908_MYC_555.json': 1, '13911_MYC_520.json': 1, '13911_MYC_533.json': 1, '13912_MYC_530.json': 0, '13913_MYC_536.json': 0, '13913_MYC_554.json': 0, '13914_MYC_534.json': 0, '13914_MYC_556.json': 0, '13915_MYC_516.json': 1, '13915_MYC_547.json': 1, '13917_MYC_526.json': 1, '13917_MYC_551.json': 1, '13919_MYC_518.json': 0, '13919_MYC_528.json': 0, '13920_MYC_549.json': 1, '13922_MYC_558.json': 1, '13923_MYC_525.json': 1, '13923_MYC_538.json': 1, '13924_MYC_515.json': 1, '13924_MYC_548.json': 1, '13952_MYC_2587.json': 1, '13952_MYC_2589.json': 1, '13953_MYC_2611.json': 1, '13953_MYC_2639.json': 1, '13954_MYC_2592.json': 1, '13955_MYC_2637.json': 0, '13957_MYC_2646.json': 0, '13958_MYC_2600.json': 1, '13958_MYC_2640.json': 1, '13959_MYC_2593.json': 1, '13959_MYC_2644.json': 1, '13960_MYC_2597.json': 1, '13961_MYC_2613.json': 0, '13961_MYC_2626.json': 0, '13962_MYC_2585.json': 1, '13962_MYC_2605.json': 1, '13963_MYC_2610.json': 0, '13963_MYC_2635.json': 0, '13964_MYC_2632.json': 1, '13965_MYC_2604.json': 1, '13965_MYC_2606.json': 1, '13966_MYC_2603.json': 1, '13966_MYC_2617.json': 1, '13971_MYC_2596.json': 1, '13971_MYC_2634.json': 1, '13972_MYC_2623.json': 1, '13972_MYC_2636.json': 1, '13973_MYC_2619.json': 1, '13973_MYC_2641.json': 1, '13974_MYC_2588.json': 1, '13974_MYC_2633.json': 1, '13976_MYC_2586.json': 1, '13976_MYC_2620.json': 1, '13977_MYC_2599.json': 1, '13977_MYC_2629.json': 1, '13978_MYC_2608.json': 1, '13978_MYC_2621.json': 1, '13979_MYC_2618.json': 0, '13979_MYC_2631.json': 0, '13980_MYC_2591.json': 0, '13980_MYC_2622.json': 0, '13981_MYC_2609.json': 1, '13981_MYC_2615.json': 1, '13983_MYC_2594.json': 1, '13983_MYC_2627.json': 1, '13986_MYC_2607.json': 1, '13986_MYC_2625.json': 1, '13988_MYC_2630.json': 1, '13989_MYC_2614.json': 0, '17643_MYC_2555.json': 0, '17643_MYC_2583.json': 0, '17648_MYC_2534.json': 1, '17648_MYC_2570.json': 1, '17649_MYC_2535.json': 1, '17649_MYC_2536.json': 1, '17650_MYC_2546.json': 1, '17650_MYC_2569.json': 1, '17653_MYC_2581.json': 0, '17654_MYC_2564.json': 1, '17654_MYC_2584.json': 1, '17656_MYC_2560.json': 1, '17656_MYC_2567.json': 1, '17657_MYC_2541.json': 1, '17657_MYC_2551.json': 1, '17658_MYC_2554.json': 1, '17658_MYC_2563.json': 1, '17660_MYC_2540.json': 1, '17660_MYC_2575.json': 1, '17662_MYC_2559.json': 0, '17662_MYC_2562.json': 0, '17664_MYC_2552.json': 1, '17664_MYC_2579.json': 1, '17666_MYC_2543.json': 0, '17666_MYC_2548.json': 0, '17667_MYC_2532.json': 1, '17667_MYC_2537.json': 1, '17669_MYC_2574.json': 1, '17670_MYC_2539.json': 1, '17670_MYC_2572.json': 1, '17673_MYC_2557.json': 1, '17673_MYC_2571.json': 1, '18828_MYC_2347.json': 1, '18828_MYC_2352.json': 1, '18829_MYC_2350.json': 1, '18829_MYC_2354.json': 1, '18830_MYC_2341.json': 1, '18830_MYC_2358.json': 1, '18831_MYC_2316.json': 1, '18831_MYC_2344.json': 1, '18831_MYC_2364.json': 1, '18833_MYC_2355.json': 0, '18834_MYC_2336.json': 0, '18834_MYC_2346.json': 0, '18835_MYC_2361.json': 0, '18835_MYC_2367.json': 0, '18836_MYC_2315.json': 1, '18836_MYC_2351.json': 1, '18837_MYC_2327.json': 0, '18837_MYC_2373.json': 0, '18838_MYC_2342.json': 1, '18838_MYC_2366.json': 1, '18839_MYC_2339.json': 0, '18840_MYC_2319.json': 1, '18840_MYC_2356.json': 1, '18841_MYC_2343.json': 1, '18843_MYC_2330.json': 0, '18843_MYC_2370.json': 0, '18844_MYC_2357.json': 1, '18845_MYC_2332.json': 1, '18845_MYC_2353.json': 1, '18846_MYC_2320.json': 1, '18846_MYC_2338.json': 1, '18847_MYC_2321.json': 1, '18847_MYC_2362.json': 1, '18848_MYC_2349.json': 0, '18848_MYC_2374.json': 0, '18849_MYC_2314.json': 0, '18849_MYC_2337.json': 0, '18851_MYC_2363.json': 0, '18851_MYC_2372.json': 0, '18852_MYC_2325.json': 0, '18852_MYC_2334.json': 0, '19667_MYC_2265.json': 0, '19667_MYC_2276.json': 0, '19669_MYC_2252.json': 1, '19669_MYC_2286.json': 1, '19670_MYC_2291.json': 1, '19670_MYC_2293.json': 1, '19672_MYC_2254.json': 1, '19672_MYC_2304.json': 1, '19673_MYC_2259.json': 0, '19673_MYC_2269.json': 0, '19674_MYC_2264.json': 0, '19674_MYC_2303.json': 0, '19675_MYC_2277.json': 1, '19675_MYC_2309.json': 1, '19676_MYC_2262.json': 1, '19676_MYC_2308.json': 1, '19677_MYC_2255.json': 0, '19678_MYC_2287.json': 1, '19679_MYC_2275.json': 1, '19679_MYC_2290.json': 1, '19681_MYC_2257.json': 1, '19681_MYC_2310.json': 1, '19682_MYC_2296.json': 0, '19682_MYC_2301.json': 0, '19683_MYC_2253.json': 1, '19683_MYC_2271.json': 1, '19684_MYC_2272.json': 1, '19684_MYC_2285.json': 1, '19685_MYC_2280.json': 1, '19685_MYC_2289.json': 1, '19687_MYC_2292.json': 1, '19687_MYC_2297.json': 1, '19688_MYC_2288.json': 0, '19688_MYC_2313.json': 0, '19689_MYC_2298.json': 1, '19689_MYC_2299.json': 1, '19690_MYC_2273.json': 0, '19690_MYC_2294.json': 0, '19691_MYC_2282.json': 0, '19691_MYC_2284.json': 0, '19692_MYC_2279.json': 0, '19692_MYC_2281.json': 0, '19693_MYC_2260.json': 0, '19693_MYC_2278.json': 0, '19694_MYC_2283.json': 0, '19694_MYC_2302.json': 0, '19695_MYC_2270.json': 0, '19695_MYC_2295.json': 0, '19697_MYC_2256.json': 0, '19697_MYC_2274.json': 0, '19698_MYC_2300.json': 1, '19698_MYC_2306.json': 1, '19700_MYC_2258.json': 0, '19700_MYC_2268.json': 0, '26786_MYC_1916.json': 0, '26786_MYC_1961.json': 0, '26787_MYC_1941.json': 0, '26787_MYC_1944.json': 0, '26788_MYC_1925.json': 1, '26788_MYC_1937.json': 1, '26789_MYC_1934.json': 1, '26789_MYC_1949.json': 1, '26790_MYC_1939.json': 0, '26790_MYC_1969.json': 0, '26791_MYC_1907.json': 0, '26791_MYC_1951.json': 0, '26792_MYC_1918.json': 0, '26793_MYC_1922.json': 0, '26793_MYC_1968.json': 0, '26794_MYC_1909.json': 1, '26794_MYC_1936.json': 1, '26795_MYC_1928.json': 1, '26795_MYC_1959.json': 1, '26796_MYC_1912.json': 1, '26796_MYC_1931.json': 1, '26797_MYC_1914.json': 0, '26798_MYC_1908.json': 0, '26798_MYC_1947.json': 0, '26799_MYC_1943.json': 0, '26799_MYC_1963.json': 0, '26800_MYC_1970.json': 0, '26801_MYC_1921.json': 0, '26801_MYC_1957.json': 0, '26802_MYC_1940.json': 0, '26802_MYC_1948.json': 0, '26803_MYC_1924.json': 0, '26803_MYC_1932.json': 0, '26804_MYC_1929.json': 1, '26804_MYC_1971.json': 1, '26805_MYC_1913.json': 1, '26805_MYC_1923.json': 1, '26806_MYC_1966.json': 0, '26808_MYC_1935.json': 0, '26808_MYC_1942.json': 0, '26809_MYC_1933.json': 0, '26809_MYC_1956.json': 0, '26810_MYC_1926.json': 0, '26810_MYC_1964.json': 0, '26811_MYC_1952.json': 0, '26812_MYC_1930.json': 0, '26812_MYC_1960.json': 0, '26813_MYC_629.json': 0, '26814_MYC_599.json': 0, '26816_MYC_605.json': 0, '26816_MYC_610.json': 0, '26817_MYC_617.json': 0, '26817_MYC_631.json': 0, '26818_MYC_583.json': 0, '26819_MYC_575.json': 1, '26819_MYC_634.json': 1, '26820_MYC_611.json': 0, '26820_MYC_618.json': 0, '26821_MYC_600.json': 0, '26821_MYC_603.json': 0, '26822_MYC_566.json': 1, '26822_MYC_628.json': 1, '26823_MYC_602.json': 0, '26823_MYC_626.json': 0, '26824_MYC_604.json': 0, '26825_MYC_580.json': 0, '26825_MYC_633.json': 0, '26826_MYC_582.json': 0, '26828_MYC_607.json': 0, '26828_MYC_616.json': 0, '26829_MYC_581.json': 0, '26830_MYC_563.json': 1, '26830_MYC_585.json': 1, '26831_MYC_591.json': 1, '26832_MYC_615.json': 0, '26832_MYC_622.json': 0, '26833_MYC_564.json': 0, '26833_MYC_567.json': 0, '26834_MYC_587.json': 0, '26834_MYC_635.json': 0, '26835_MYC_592.json': 0, '26836_MYC_570.json': 0, '26838_MYC_574.json': 0, '26838_MYC_578.json': 0, '26839_MYC_593.json': 0, '26839_MYC_624.json': 0, '26841_MYC_595.json': 0, '26841_MYC_608.json': 0, '26842_MYC_573.json': 1, '26842_MYC_586.json': 1, '26843_MYC_612.json': 0, '26843_MYC_614.json': 0, '26844_MYC_609.json': 0, '26845_MYC_565.json': 0, '26845_MYC_625.json': 0, '26846_MYC_571.json': 0, '26846_MYC_623.json': 0, '26847_MYC_568.json': 0, '26847_MYC_630.json': 0, '26848_MYC_584.json': 0, '26848_MYC_594.json': 0, '26849_MYC_598.json': 0, '26849_MYC_619.json': 0, '26850_MYC_576.json': 0, '26850_MYC_620.json': 0, '26851_MYC_589.json': 1, '26851_MYC_596.json': 1, '26853_MYC_579.json': 0, '26853_MYC_597.json': 0, '26857_MYC_561.json': 0, '26858_MYC_601.json': 1, '26858_MYC_606.json': 1, '26859_MYC_572.json': 1, '26859_MYC_627.json': 1}\n",
      "['13901_MYC_532' '13901_MYC_557' '13902_MYC_513' '13902_MYC_514'\n",
      " '13903_MYC_535' '13903_MYC_560' '13904_MYC_539' '13904_MYC_550'\n",
      " '13908_MYC_527' '13908_MYC_555' '13911_MYC_520' '13911_MYC_533'\n",
      " '13912_MYC_530' '13913_MYC_536' '13913_MYC_554' '13914_MYC_534'\n",
      " '13914_MYC_556' '13915_MYC_516' '13915_MYC_547' '13917_MYC_526'\n",
      " '13917_MYC_551' '13919_MYC_518' '13919_MYC_528' '13920_MYC_549'\n",
      " '13922_MYC_558' '13923_MYC_525' '13923_MYC_538' '13924_MYC_515'\n",
      " '13924_MYC_548' '13952_MYC_2587' '13952_MYC_2589' '13953_MYC_2611'\n",
      " '13953_MYC_2639' '13954_MYC_2592' '13955_MYC_2637' '13957_MYC_2646'\n",
      " '13958_MYC_2600' '13958_MYC_2640' '13959_MYC_2593' '13959_MYC_2644'\n",
      " '13960_MYC_2597' '13961_MYC_2613' '13961_MYC_2626' '13962_MYC_2585'\n",
      " '13962_MYC_2605' '13963_MYC_2610' '13963_MYC_2635' '13964_MYC_2632'\n",
      " '13965_MYC_2604' '13965_MYC_2606' '13966_MYC_2603' '13966_MYC_2617'\n",
      " '13971_MYC_2596' '13971_MYC_2634' '13972_MYC_2623' '13972_MYC_2636'\n",
      " '13973_MYC_2619' '13973_MYC_2641' '13974_MYC_2588' '13974_MYC_2633'\n",
      " '13976_MYC_2586' '13976_MYC_2620' '13977_MYC_2599' '13977_MYC_2629'\n",
      " '13978_MYC_2608' '13978_MYC_2621' '13979_MYC_2618' '13979_MYC_2631'\n",
      " '13980_MYC_2591' '13980_MYC_2622' '13981_MYC_2609' '13981_MYC_2615'\n",
      " '13983_MYC_2594' '13983_MYC_2627' '13986_MYC_2607' '13986_MYC_2625'\n",
      " '13988_MYC_2630' '13989_MYC_2614' '17643_MYC_2555' '17643_MYC_2583'\n",
      " '17648_MYC_2534' '17648_MYC_2570' '17649_MYC_2535' '17649_MYC_2536'\n",
      " '17650_MYC_2546' '17650_MYC_2569' '17653_MYC_2581' '17654_MYC_2564'\n",
      " '17654_MYC_2584' '17656_MYC_2560' '17656_MYC_2567' '17657_MYC_2541'\n",
      " '17657_MYC_2551' '17658_MYC_2554' '17658_MYC_2563' '17660_MYC_2540'\n",
      " '17660_MYC_2575' '17662_MYC_2559' '17662_MYC_2562' '17664_MYC_2552'\n",
      " '17664_MYC_2579' '17666_MYC_2543' '17666_MYC_2548' '17667_MYC_2532'\n",
      " '17667_MYC_2537' '17669_MYC_2574' '17670_MYC_2539' '17670_MYC_2572'\n",
      " '17673_MYC_2557' '17673_MYC_2571' '18828_MYC_2347' '18828_MYC_2352'\n",
      " '18829_MYC_2350' '18829_MYC_2354' '18830_MYC_2341' '18830_MYC_2358'\n",
      " '18831_MYC_2316' '18831_MYC_2344' '18831_MYC_2364' '18833_MYC_2355'\n",
      " '18834_MYC_2336' '18834_MYC_2346' '18835_MYC_2361' '18835_MYC_2367'\n",
      " '18836_MYC_2315' '18836_MYC_2351' '18837_MYC_2327' '18837_MYC_2373'\n",
      " '18838_MYC_2342' '18838_MYC_2366' '18839_MYC_2339' '18840_MYC_2319'\n",
      " '18840_MYC_2356' '18841_MYC_2343' '18843_MYC_2330' '18843_MYC_2370'\n",
      " '18844_MYC_2357' '18845_MYC_2332' '18845_MYC_2353' '18846_MYC_2320'\n",
      " '18846_MYC_2338' '18847_MYC_2321' '18847_MYC_2362' '18848_MYC_2349'\n",
      " '18848_MYC_2374' '18849_MYC_2314' '18849_MYC_2337' '18851_MYC_2363'\n",
      " '18851_MYC_2372' '18852_MYC_2325' '18852_MYC_2334' '19667_MYC_2265'\n",
      " '19667_MYC_2276' '19669_MYC_2252' '19669_MYC_2286' '19670_MYC_2291'\n",
      " '19670_MYC_2293' '19672_MYC_2254' '19672_MYC_2304' '19673_MYC_2259'\n",
      " '19673_MYC_2269' '19674_MYC_2264' '19674_MYC_2303' '19675_MYC_2277'\n",
      " '19675_MYC_2309' '19676_MYC_2262' '19676_MYC_2308' '19677_MYC_2255'\n",
      " '19678_MYC_2287' '19679_MYC_2275' '19679_MYC_2290' '19681_MYC_2257'\n",
      " '19681_MYC_2310' '19682_MYC_2296' '19682_MYC_2301' '19683_MYC_2253'\n",
      " '19683_MYC_2271' '19684_MYC_2272' '19684_MYC_2285' '19685_MYC_2280'\n",
      " '19685_MYC_2289' '19687_MYC_2292' '19687_MYC_2297' '19688_MYC_2288'\n",
      " '19688_MYC_2313' '19689_MYC_2298' '19689_MYC_2299' '19690_MYC_2273'\n",
      " '19690_MYC_2294' '19691_MYC_2282' '19691_MYC_2284' '19692_MYC_2279'\n",
      " '19692_MYC_2281' '19693_MYC_2260' '19693_MYC_2278' '19694_MYC_2283'\n",
      " '19694_MYC_2302' '19695_MYC_2270' '19695_MYC_2295' '19697_MYC_2256'\n",
      " '19697_MYC_2274' '19698_MYC_2300' '19698_MYC_2306' '19700_MYC_2258'\n",
      " '19700_MYC_2268' '26786_MYC_1916' '26786_MYC_1961' '26787_MYC_1941'\n",
      " '26787_MYC_1944' '26788_MYC_1925' '26788_MYC_1937' '26789_MYC_1934'\n",
      " '26789_MYC_1949' '26790_MYC_1939' '26790_MYC_1969' '26791_MYC_1907'\n",
      " '26791_MYC_1951' '26792_MYC_1918' '26793_MYC_1922' '26793_MYC_1968'\n",
      " '26794_MYC_1909' '26794_MYC_1936' '26795_MYC_1928' '26795_MYC_1959'\n",
      " '26796_MYC_1912' '26796_MYC_1931' '26797_MYC_1914' '26798_MYC_1908'\n",
      " '26798_MYC_1947' '26799_MYC_1943' '26799_MYC_1963' '26800_MYC_1970'\n",
      " '26801_MYC_1921' '26801_MYC_1957' '26802_MYC_1940' '26802_MYC_1948'\n",
      " '26803_MYC_1924' '26803_MYC_1932' '26804_MYC_1929' '26804_MYC_1971'\n",
      " '26805_MYC_1913' '26805_MYC_1923' '26806_MYC_1966' '26808_MYC_1935'\n",
      " '26808_MYC_1942' '26809_MYC_1933' '26809_MYC_1956' '26810_MYC_1926'\n",
      " '26810_MYC_1964' '26811_MYC_1952' '26812_MYC_1930' '26812_MYC_1960'\n",
      " '26813_MYC_629' '26814_MYC_599' '26816_MYC_605' '26816_MYC_610'\n",
      " '26817_MYC_617' '26817_MYC_631' '26818_MYC_583' '26819_MYC_575'\n",
      " '26819_MYC_634' '26820_MYC_611' '26820_MYC_618' '26821_MYC_600'\n",
      " '26821_MYC_603' '26822_MYC_566' '26822_MYC_628' '26823_MYC_602'\n",
      " '26823_MYC_626' '26824_MYC_604' '26825_MYC_580' '26825_MYC_633'\n",
      " '26826_MYC_582' '26828_MYC_607' '26828_MYC_616' '26829_MYC_581'\n",
      " '26830_MYC_563' '26830_MYC_585' '26831_MYC_591' '26832_MYC_615'\n",
      " '26832_MYC_622' '26833_MYC_564' '26833_MYC_567' '26834_MYC_587'\n",
      " '26834_MYC_635' '26835_MYC_592' '26836_MYC_570' '26838_MYC_574'\n",
      " '26838_MYC_578' '26839_MYC_593' '26839_MYC_624' '26841_MYC_595'\n",
      " '26841_MYC_608' '26842_MYC_573' '26842_MYC_586' '26843_MYC_612'\n",
      " '26843_MYC_614' '26844_MYC_609' '26845_MYC_565' '26845_MYC_625'\n",
      " '26846_MYC_571' '26846_MYC_623' '26847_MYC_568' '26847_MYC_630'\n",
      " '26848_MYC_584' '26848_MYC_594' '26849_MYC_598' '26849_MYC_619'\n",
      " '26850_MYC_576' '26850_MYC_620' '26851_MYC_589' '26851_MYC_596'\n",
      " '26853_MYC_579' '26853_MYC_597' '26857_MYC_561' '26858_MYC_601'\n",
      " '26858_MYC_606' '26859_MYC_572' '26859_MYC_627'] [0 0 1 1 1 1 0 0 1 1 1 1 0 0 0 0 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1\n",
      " 1 1 1 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1\n",
      " 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 0 0 0\n",
      " 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0\n",
      " 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 1\n",
      " 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/home/amrit/data/proj_data/MLG_project/DLBCL-Morph/training/splits_MYC_OS.dat']"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.dset import multilabel_stratified_split\n",
    "\n",
    "NUM_FOLDS = 1\n",
    "TEST_RATIO = 0.2\n",
    "TRAIN_RATIO = 0.8 * 0.7\n",
    "VALID_RATIO = 0.8 * 0.3\n",
    "\n",
    "# if SPLIT_PATH and os.path.exists(SPLIT_PATH):\n",
    "#     splits = joblib.load(SPLIT_PATH)\n",
    "# else:\n",
    "x = np.array(wsi_names)\n",
    "with open(LABELSPATH, 'r') as f:\n",
    "    labels_dict = json.load(f)\n",
    "    print(labels_dict)\n",
    "print(labels_dict)\n",
    "y = np.array([labels_dict[wsi_name+'.json'] for wsi_name in wsi_names])\n",
    "# y[np.where(y==-1)] = 0\n",
    "\n",
    "# splits = multilabel_stratified_split(x, y, TRAIN_RATIO, VALID_RATIO, TEST_RATIO, NUM_FOLDS)\n",
    "\n",
    "if LABEL_TYPE == \"multilabel\":\n",
    "    splits = multilabel_stratified_split(x, y, TRAIN_RATIO, VALID_RATIO, TEST_RATIO, NUM_FOLDS)\n",
    "else:\n",
    "    splits = stratified_split(x, y, TRAIN_RATIO, VALID_RATIO, TEST_RATIO, NUM_FOLDS)\n",
    "\n",
    "joblib.dump(splits, SPLIT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/amrit/data/proj_data/MLG_project/DLBCL-Morph/training/splits_MYC_OS.dat')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPLIT_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Callable\n",
    "import ujson as json\n",
    "import numpy as np\n",
    "import pathlib\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Sampler\n",
    "from torch_geometric.data import Data, Dataset\n",
    "\n",
    "\n",
    "class SlideGraphDataset_surv(Dataset):\n",
    "    \"\"\"Handling loading graph data from disk.\n",
    "\n",
    "    Args:\n",
    "        info_list (list): In case of `train` or `valid` is in `mode`,\n",
    "            this is expected to be a list of `[uid, label]` . Otherwise,\n",
    "            it is a list of `uid`. Here, `uid` is used to construct\n",
    "            `f\"{GRAPH_DIR}/{wsi_code}.json\"` which is a path points to\n",
    "            a `.json` file containing the graph structure. By `label`, we mean\n",
    "            the label of the graph. The format within the `.json` file comes\n",
    "            from `tiatoolbox.tools.graph`.\n",
    "        mode (str): This denotes which data mode the `info_list` is in.\n",
    "        preproc (callable): The prerocessing function for each node\n",
    "            within the graph.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self: Dataset,\n",
    "        info_list: list,\n",
    "        mode: str = \"train\",\n",
    "        graph_dir: pathlib.Path = None,\n",
    "        preproc: Callable | None = None,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize SlideGraphDataset.\"\"\"\n",
    "        self.info_list = info_list\n",
    "        self.mode = mode\n",
    "        self.graph_dir = graph_dir\n",
    "        self.preproc = preproc\n",
    "\n",
    "    def __getitem__(self: Dataset, idx: int) -> Dataset:\n",
    "        \"\"\"Get an element from SlideGraphDataset.\"\"\"\n",
    "        info = self.info_list[idx]\n",
    "        if any(v in self.mode for v in [\"train\", \"valid\"]):\n",
    "            wsi_code, label = info\n",
    "            # torch.Tensor will create 1-d vector not scalar\n",
    "            label = torch.tensor(label)\n",
    "        else:\n",
    "            wsi_code = info\n",
    "\n",
    "        with (self.graph_dir / f\"{wsi_code}.json\").open() as fptr:\n",
    "            graph_dict = json.load(fptr)\n",
    "\n",
    "        surv_event =  graph_dict.pop(\"surv_event\")\n",
    "        surv_time =  graph_dict.pop(\"surv_time\")\n",
    "\n",
    "        \n",
    "        graph_dict = {k: np.array(v) for k, v in graph_dict.items()}\n",
    "\n",
    "        if self.preproc is not None:\n",
    "            graph_dict[\"x\"] = self.preproc(graph_dict[\"x\"])\n",
    "\n",
    "        graph_dict = {k: torch.tensor(v) for k, v in graph_dict.items()}\n",
    "        graph = Data(**graph_dict)\n",
    "\n",
    "        if any(v in self.mode for v in [\"train\", \"valid\"]):\n",
    "            return {\"graph\": graph, \"label\": label, \"surv_event\" : surv_event, \"surv_time\" : surv_time}\n",
    "        return {\"graph\": graph}\n",
    "\n",
    "    def __len__(self: Dataset) -> int:\n",
    "        \"\"\"Length of SlideGraphDataset.\"\"\"\n",
    "        return len(self.info_list)\n",
    "\n",
    "    def len(self):\n",
    "        return self.__len__()\n",
    "\n",
    "    def get(self, idx):\n",
    "        return self.__getitem__(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'graph': Data(x=[95, 128], edge_index=[2, 506], y=1, coordinates=[95, 2]),\n",
       " 'label': tensor(1),\n",
       " 'surv_event': [0],\n",
       " 'surv_time': [12.72]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'graph': DataBatch(x=[2125, 128], edge_index=[2, 11016], y=[32], coordinates=[2125, 2], batch=[2125], ptr=[33]), 'label': tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,\n",
      "        1, 0, 1, 1, 0, 1, 0, 1]), 'surv_event': [tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,\n",
      "        1, 1, 0, 0, 1, 0, 1, 0])], 'surv_time': [tensor([ 8.1100, 10.5200,  2.7800, 12.7200,  4.7800,  9.0700,  8.2700,  7.4200,\n",
      "         8.5400,  6.5900,  7.7800,  8.6300,  4.4000,  4.2300,  8.3600,  8.3400,\n",
      "         7.6200,  3.8100,  9.0800, 11.6700,  8.3300,  9.8400,  1.3000,  5.4100,\n",
      "        10.8900,  3.4400,  8.4400,  8.7200,  2.0600,  8.0400,  4.9000, 13.8800])]}\n"
     ]
    }
   ],
   "source": [
    "from src.utils import load_json, rm_n_mkdir, mkdir, recur_find_ext\n",
    "from src.dset import SlideGraphDataset, stratified_split, StratifiedSampler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "nodes_preproc_func = None\n",
    "\n",
    "def sample_data(  # noqa: C901, PLR0912, PLR0915\n",
    "    dataset_dict: dict,\n",
    "    GRAPH_DIR = None) -> list:\n",
    "    \n",
    "    for subset_name, subset in dataset_dict.items():\n",
    "        ds = SlideGraphDataset_surv(subset, mode=\"train\", preproc=nodes_preproc_func, graph_dir=GRAPH_DIR)\n",
    "\n",
    "        batch_sampler = None\n",
    "\n",
    "        _loader_kwargs = {\n",
    "            \"num_workers\": 6,\n",
    "            \"batch_size\": 32,\n",
    "        }\n",
    "        # # arch_kwargs = {\n",
    "\n",
    "        # print(\"subset\" ,subset_name , len(subset))\n",
    "        \n",
    "        # batch_sampler = batch_sampler = StratifiedSampler(\n",
    "        #             labels=[v[1] for v in subset],\n",
    "        #             batch_size=32,\n",
    "        #         )\n",
    "\n",
    "        loader =  DataLoader(ds,\n",
    "                    batch_sampler=batch_sampler,\n",
    "            drop_last=subset_name == \"train\" and batch_sampler is None,\n",
    "            shuffle=subset_name == \"train\" and batch_sampler is None,\n",
    "            **_loader_kwargs,)\n",
    "        \n",
    "    return ds, loader\n",
    "\n",
    "\n",
    "for split_idx, split in enumerate(splits):\n",
    "    new_split = {\n",
    "                \"train\": split[\"train\"],\n",
    "                \"infer-train\": split[\"train\"],\n",
    "                \"infer-valid-A\": split[\"valid\"],\n",
    "                \"infer-valid-B\": split[\"test\"],\n",
    "            }\n",
    "    ds, loader = sample_data(new_split, GRAPHSDIR)\n",
    "\n",
    "sample = ds.__getitem__(3)\n",
    "display(sample)\n",
    "\n",
    "for _step, batch_data in enumerate(loader):\n",
    "    print(batch_data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Import modules required to run the Jupyter notebook.\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "# Clear logger to use tiatoolbox.logger\n",
    "import logging\n",
    "\n",
    "if logging.getLogger().hasHandlers():\n",
    "    logging.getLogger().handlers.clear()\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    ")\n",
    "\n",
    "import copy\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Third party imports\n",
    "import joblib\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from sklearn import metrics\n",
    "\n",
    "import ujson as json\n",
    "from torch_geometric.loader import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tiatoolbox import logger\n",
    "\n",
    "from tiatoolbox.utils.misc import save_as_json\n",
    "\n",
    "\n",
    "from src.utils import load_json, rm_n_mkdir, mkdir, recur_find_ext\n",
    "from src.dset import SlideGraphDataset, stratified_split, StratifiedSampler, StratifiedSampler_multilabel\n",
    "from src.model import SlideGraphArch\n",
    "from src.utils import ScalarMovingAverage\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "mpl.rcParams[\"figure.dpi\"] = 300  # for high resolution figure in notebook\n",
    "\n",
    "\n",
    "nodes_preproc_func = None\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def multilabel_accuracy(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Calculate multilabel classification metrics.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true: 2D array-like or binary indicator matrix, true labels\n",
    "    - y_pred: 2D array-like or binary indicator matrix, predicted labels\n",
    "\n",
    "    Returns:\n",
    "    - overall_accuracy: Subset accuracy (exact match ratio)\n",
    "    - label_accuracies: List of accuracies for each individual label\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_pred[y_pred > 0.5] = 1\n",
    "    y_pred[y_pred < 0.5] = 0\n",
    "\n",
    "    # Ensure the shapes are the same\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(\"Shapes of y_true and y_pred must be the same.\")\n",
    "    \n",
    "    label_accuracies = np.mean(y_true == y_pred, axis=0).round(4)\n",
    "    overall_accuracy = np.mean(label_accuracies).round(4)\n",
    "\n",
    "    return overall_accuracy, label_accuracies\n",
    "\n",
    "def run_once(  # noqa: C901, PLR0912, PLR0915\n",
    "    dataset_dict: dict,\n",
    "    num_epochs: int,\n",
    "    save_dir: str | Path,\n",
    "    pretrained: str | None = None,\n",
    "    loader_kwargs: dict | None = None,\n",
    "    arch_kwargs: dict | None = None,\n",
    "    optim_kwargs: dict | None = None,\n",
    "    *,\n",
    "    on_gpu: bool = True,\n",
    "    GRAPH_DIR = None,\n",
    "    LABEL_TYPE = None\n",
    ") -> list:\n",
    "    \"\"\"Running the inference or training loop once.\n",
    "\n",
    "    The actual running mode is defined via the code name of the dataset\n",
    "    within `dataset_dict`. Here, `train` is specifically preserved for\n",
    "    the dataset used for training. `.*infer-valid.*` and `.*infer-train*`\n",
    "    are reserved for datasets containing the corresponding labels.\n",
    "    Otherwise, the dataset is assumed to be for the inference run.\n",
    "\n",
    "    \"\"\"\n",
    "    if loader_kwargs is None:\n",
    "        loader_kwargs = {}\n",
    "\n",
    "    if arch_kwargs is None:\n",
    "        arch_kwargs = {}\n",
    "\n",
    "    if optim_kwargs is None:\n",
    "        optim_kwargs = {}\n",
    "\n",
    "    if on_gpu == True:\n",
    "        device = \"cuda\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "\n",
    "    model = SlideGraphArch(**arch_kwargs)\n",
    "    print(model)\n",
    "    if pretrained is not None:\n",
    "        model.load(*pretrained)\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), **optim_kwargs)\n",
    "\n",
    "    # Create the graph dataset holder for each subset info then\n",
    "    # pipe them through torch/torch geometric specific loader\n",
    "    # for loading in multi-thread.\n",
    "    loader_dict = {}\n",
    "    for subset_name, subset in dataset_dict.items():\n",
    "        _loader_kwargs = copy.deepcopy(loader_kwargs)\n",
    "        batch_sampler = None\n",
    "        if subset_name == \"train\":\n",
    "            _loader_kwargs = {}\n",
    "            if LABEL_TYPE == 'multilabel':\n",
    "                batch_sampler = StratifiedSampler_multilabel(\n",
    "                    labels=[v[1] for v in subset],\n",
    "                    batch_size=loader_kwargs[\"batch_size\"],\n",
    "                )\n",
    "            else:\n",
    "                batch_sampler = StratifiedSampler(\n",
    "                    labels=[v[1] for v in subset],\n",
    "                    batch_size=loader_kwargs[\"batch_size\"],\n",
    "                )\n",
    "\n",
    "        ds = SlideGraphDataset_surv(subset, mode=subset_name, preproc=nodes_preproc_func, graph_dir=GRAPH_DIR)\n",
    "        loader_dict[subset_name] = DataLoader(\n",
    "            ds,\n",
    "            batch_sampler=batch_sampler,\n",
    "            drop_last=subset_name == \"train\" and batch_sampler is None,\n",
    "            shuffle=subset_name == \"train\" and batch_sampler is None,\n",
    "            **_loader_kwargs,\n",
    "        )\n",
    "    best_score = {}\n",
    "    best_score_num = {}\n",
    "    best_score[f\"infer-train-accuracy\"] = 0\n",
    "    best_score[f\"infer-valid-A-accuracy\"] = 0\n",
    "    best_score[f\"infer-valid-B-accuracy\"] = 0\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        logger.info(\"EPOCH: %03d\", epoch)\n",
    "        for loader_name, loader in loader_dict.items():\n",
    "            # * EPOCH START\n",
    "            step_output = []\n",
    "            step_surv_event= []\n",
    "            step_surv_time= []\n",
    "\n",
    "\n",
    "            ema = ScalarMovingAverage()\n",
    "            for _step, batch_data in enumerate(tqdm(loader, disable=loader_name!=\"train\")):\n",
    "                # * STEP COMPLETE CALLBACKS\n",
    "                if loader_name == \"train\":\n",
    "                    outputs = model.train_batch(model, batch_data, optimizer, on_gpu=on_gpu)\n",
    "                    ema({\"loss\": outputs[0]})\n",
    "                else:\n",
    "                    output = model.infer_batch(model, batch_data, on_gpu=on_gpu)\n",
    "\n",
    "                    batch_size = batch_data[\"graph\"].num_graphs\n",
    "                    \n",
    "                    surv_event_batch = batch_data[\"surv_event\"]\n",
    "                    surv_time_batch = batch_data[\"surv_time\"]\n",
    "\n",
    "\n",
    "                    #print(\"surv_label_batch\" , surv_label_batch)\n",
    "                    # Iterate over output head and retrieve\n",
    "                    # each as N x item, each item may be of\n",
    "                    # arbitrary dimensions\n",
    "                    output = [np.split(v, batch_size, axis=0) for v in output]\n",
    "                    # pairing such that it will be\n",
    "                    # N batch size x H head list\n",
    "                    output = list(zip(*output))\n",
    "                    step_output.extend(output)\n",
    "\n",
    "\n",
    "                    surv_event_batch = list(zip(*surv_event_batch))\n",
    "                    step_surv_event.extend(surv_event_batch)\n",
    "\n",
    "                    surv_time_batch = list(zip(*surv_time_batch))\n",
    "                    step_surv_time.extend(surv_time_batch)\n",
    "                    #print(\"step_surv_label\" , step_surv_label)\n",
    "\n",
    "\n",
    "                # pbar.update()\n",
    "            # pbar.close()\n",
    "\n",
    "            # * EPOCH COMPLETE\n",
    "\n",
    "            # Callbacks to process output\n",
    "            logging_dict = {}\n",
    "            \n",
    "            if loader_name == \"train\":\n",
    "                for val_name, val in ema.tracking_dict.items():\n",
    "                    logging_dict[f\"train-EMA-{val_name}\"] = val.item()\n",
    "            elif \"infer\" in loader_name and any(v in loader_name for v in [\"train\", \"valid\"]):\n",
    "                # Expand the list of N dataset size x H heads\n",
    "                # back to a list of H Head each with N samples.\n",
    "                output = list(zip(*step_output))\n",
    "                pred, gtruth = output\n",
    "                pred = np.squeeze(np.array(pred))\n",
    "                gtruth = np.squeeze(np.array(gtruth))\n",
    "                \n",
    "                surv_event_list = list(zip(*step_surv_event))\n",
    "                surv_event_list = np.squeeze(np.array(surv_event_list))\n",
    "                # print(\"pred\",loader_name , pred.shape)\n",
    "                # print(\"gtruth\",loader_name , gtruth.shape)\n",
    "                # print(\"surv_event_list\",loader_name , surv_event_list.shape)\n",
    "\n",
    "                surv_time_list = list(zip(*step_surv_time))\n",
    "                surv_time_list = np.squeeze(np.array(surv_time_list))\n",
    "                # print(\"surv_time_list\" ,loader_name, surv_time_list.shape)\n",
    "\n",
    "                # if \"valid\" in loader_name:\n",
    "                #     print(loader_name, pred) # gtruth)\n",
    "\n",
    "                if LABEL_TYPE == 'multilabel':\n",
    "                    # print(pred, gtruth)\n",
    "                    curr_score, label_accuracies = multilabel_accuracy(pred, gtruth)\n",
    "                    logging_dict[f\"{loader_name}-accuracy\"] = float(curr_score)\n",
    "                    # print(loader_name , \"label_accuracies\" , label_accuracies)\n",
    "                    # logging_dict[f\"{loader_name}-individual_accuracy\"] = float(label_accuracies)\n",
    "                else:\n",
    "                    # logging_dict[f\"{loader_name}-microf1\"] = metrics.f1_score(pred, gtruth, average='micro')\n",
    "                    curr_score = round(metrics.accuracy_score(np.argmax(pred, axis=1), gtruth),3)\n",
    "                    logging_dict[f\"{loader_name}-accuracy\"] = float(curr_score)\n",
    "                # try:\n",
    "                if curr_score >= best_score[f\"{loader_name}-accuracy\"]:\n",
    "                    best_score[f\"{loader_name}-accuracy\"] = curr_score\n",
    "                    best_score[f\"{loader_name}-best_epoch\"] = epoch\n",
    "                    if LABEL_TYPE == 'multilabel':\n",
    "                        best_score[f\"{loader_name}-label_accuracies\"] = label_accuracies.tolist()\n",
    "\n",
    "                    best_score_num[f\"{loader_name}-pred\"] = pred\n",
    "                    best_score_num[f\"{loader_name}-gt\"] = gtruth\n",
    "                    best_score_num[f\"{loader_name}-surv_time_list\"] = surv_time_list\n",
    "                    best_score_num[f\"{loader_name}-surv_event_list\"] = surv_event_list\n",
    "\n",
    "\n",
    "\n",
    "                    best_best_Score = best_score_num\n",
    "\n",
    "                # except:\n",
    "                #     best_score[f\"{loader_name}-accuracy\"] = 0\n",
    "                # logging_dict[f\"{loader_name}-raw-pred\"] = pred\n",
    "                # logging_dict[f\"{loader_name}-raw-gtruth\"] = gtruth\n",
    "\n",
    "            # Callbacks for logging and saving\n",
    "            for val_name, val in logging_dict.items():\n",
    "                if \"raw\" not in val_name:\n",
    "                    logging.info(\"%s: %f\", val_name, val)\n",
    "            if \"train\" not in loader_dict:\n",
    "                continue\n",
    "\n",
    "            # Track the statistics\n",
    "            new_stats = {}\n",
    "            if (save_dir / \"stats.json\").exists():\n",
    "                old_stats = load_json(save_dir/\"stats.json\")\n",
    "                # Save a backup first\n",
    "                save_as_json(old_stats, save_dir/\"stats.old.json\", exist_ok=True)\n",
    "                new_stats = copy.deepcopy(old_stats)\n",
    "                # new_stats = {int(k.value()): v for k, v in new_stats.items()}\n",
    "                new_stats = {int(k): v for k, v in new_stats.items()}\n",
    "\n",
    "            old_epoch_stats = {}\n",
    "            if epoch in new_stats:\n",
    "                old_epoch_stats = new_stats[epoch]\n",
    "            old_epoch_stats.update(logging_dict)\n",
    "            new_stats[epoch] = old_epoch_stats\n",
    "            save_as_json(new_stats, save_dir/\"stats.json\", exist_ok=True)\n",
    "\n",
    "            # Save the dictionary to a JSON file\n",
    "            with open(save_dir/\"best_score.json\", 'w') as json_file:\n",
    "                json.dump(best_score, json_file, indent=4)\n",
    "\n",
    "        plt.figure()\n",
    "        for pkey in new_stats[0].keys():\n",
    "            vals = [new_stats[eitr][pkey] for eitr in range(epoch+1)]\n",
    "            plt.plot(np.arange(len(vals)), vals, label=pkey)\n",
    "        plt.title(\"Best_acc\" + str(list(best_score.values())))\n",
    "        plt.tight_layout()\n",
    "        plt.legend()\n",
    "        plt.savefig(save_dir/'progress.png')\n",
    "        plt.close()\n",
    "\n",
    "        if epoch % 25 == 0:\n",
    "            # Save the pytorch model\n",
    "            model.save(\n",
    "                f\"{save_dir}/epoch={epoch:03d}.weights.pth\",\n",
    "                f\"{save_dir}/epoch={epoch:03d}.aux.dat\",\n",
    "            )\n",
    "            print(\"best_score\" , best_score)\n",
    "    \n",
    "    print(best_score)\n",
    "\n",
    "    return pred, gtruth ,surv_event_list , surv_time_list , best_best_Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|2023-12-11|21:45:38.338| [INFO] EPOCH: 000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SlideGraphArch(\n",
      "  (first_h): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (nns): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=32, bias=True)\n",
      "      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=32, bias=True)\n",
      "      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (convs): ModuleList(\n",
      "    (0): EdgeConv(nn=Sequential(\n",
      "      (0): Linear(in_features=128, out_features=32, bias=True)\n",
      "      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    ))\n",
      "    (1): EdgeConv(nn=Sequential(\n",
      "      (0): Linear(in_features=64, out_features=32, bias=True)\n",
      "      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    ))\n",
      "  )\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=64, out_features=2, bias=True)\n",
      "    (1-2): 2 x Linear(in_features=32, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 12.41it/s]\n",
      "|2023-12-11|21:45:38.744| [INFO] train-EMA-loss: 0.863734\n",
      "|2023-12-11|21:45:39.127| [INFO] infer-train-accuracy: 0.500000\n",
      "|2023-12-11|21:45:39.440| [INFO] infer-valid-A-accuracy: 0.494000\n",
      "|2023-12-11|21:45:39.725| [INFO] EPOCH: 001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score {'infer-train-accuracy': 0.5, 'infer-valid-A-accuracy': 0.494, 'infer-valid-B-accuracy': 0, 'infer-train-best_epoch': 0, 'infer-valid-A-best_epoch': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 12.01it/s]\n",
      "|2023-12-11|21:45:40.144| [INFO] train-EMA-loss: 0.668020\n",
      "|2023-12-11|21:45:40.540| [INFO] infer-train-accuracy: 0.556000\n",
      "|2023-12-11|21:45:40.843| [INFO] infer-valid-A-accuracy: 0.584000\n",
      "|2023-12-11|21:45:41.135| [INFO] EPOCH: 002\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.17it/s]\n",
      "|2023-12-11|21:45:41.549| [INFO] train-EMA-loss: 0.701116\n",
      "|2023-12-11|21:45:41.915| [INFO] infer-train-accuracy: 0.601000\n",
      "|2023-12-11|21:45:42.219| [INFO] infer-valid-A-accuracy: 0.636000\n",
      "|2023-12-11|21:45:42.523| [INFO] EPOCH: 003\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.92it/s]\n",
      "|2023-12-11|21:45:42.913| [INFO] train-EMA-loss: 0.642792\n",
      "|2023-12-11|21:45:43.288| [INFO] infer-train-accuracy: 0.601000\n",
      "|2023-12-11|21:45:43.627| [INFO] infer-valid-A-accuracy: 0.597000\n",
      "|2023-12-11|21:45:43.919| [INFO] EPOCH: 004\n",
      "100%|██████████| 5/5 [00:00<00:00, 13.18it/s]\n",
      "|2023-12-11|21:45:44.301| [INFO] train-EMA-loss: 0.619204\n",
      "|2023-12-11|21:45:44.682| [INFO] infer-train-accuracy: 0.635000\n",
      "|2023-12-11|21:45:44.990| [INFO] infer-valid-A-accuracy: 0.571000\n",
      "|2023-12-11|21:45:45.299| [INFO] EPOCH: 005\n",
      "100%|██████████| 5/5 [00:00<00:00, 11.64it/s]\n",
      "|2023-12-11|21:45:45.731| [INFO] train-EMA-loss: 0.642522\n",
      "|2023-12-11|21:45:46.106| [INFO] infer-train-accuracy: 0.624000\n",
      "|2023-12-11|21:45:46.431| [INFO] infer-valid-A-accuracy: 0.532000\n",
      "|2023-12-11|21:45:46.796| [INFO] EPOCH: 006\n",
      "100%|██████████| 5/5 [00:00<00:00, 11.70it/s]\n",
      "|2023-12-11|21:45:47.226| [INFO] train-EMA-loss: 0.616103\n",
      "|2023-12-11|21:45:47.592| [INFO] infer-train-accuracy: 0.618000\n",
      "|2023-12-11|21:45:47.894| [INFO] infer-valid-A-accuracy: 0.494000\n",
      "|2023-12-11|21:45:48.190| [INFO] EPOCH: 007\n",
      "100%|██████████| 5/5 [00:00<00:00, 13.56it/s]\n",
      "|2023-12-11|21:45:48.561| [INFO] train-EMA-loss: 0.562284\n",
      "|2023-12-11|21:45:48.912| [INFO] infer-train-accuracy: 0.618000\n",
      "|2023-12-11|21:45:49.233| [INFO] infer-valid-A-accuracy: 0.519000\n",
      "|2023-12-11|21:45:49.550| [INFO] EPOCH: 008\n",
      "100%|██████████| 5/5 [00:00<00:00, 13.01it/s]\n",
      "|2023-12-11|21:45:49.937| [INFO] train-EMA-loss: 0.578495\n",
      "|2023-12-11|21:45:50.297| [INFO] infer-train-accuracy: 0.629000\n",
      "|2023-12-11|21:45:50.616| [INFO] infer-valid-A-accuracy: 0.545000\n",
      "|2023-12-11|21:45:50.912| [INFO] EPOCH: 009\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.76it/s]\n",
      "|2023-12-11|21:45:51.306| [INFO] train-EMA-loss: 0.534281\n",
      "|2023-12-11|21:45:51.697| [INFO] infer-train-accuracy: 0.640000\n",
      "|2023-12-11|21:45:51.994| [INFO] infer-valid-A-accuracy: 0.571000\n",
      "|2023-12-11|21:45:52.543| [INFO] EPOCH: 010\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.19it/s]\n",
      "|2023-12-11|21:45:52.955| [INFO] train-EMA-loss: 0.639715\n",
      "|2023-12-11|21:45:53.332| [INFO] infer-train-accuracy: 0.629000\n",
      "|2023-12-11|21:45:53.631| [INFO] infer-valid-A-accuracy: 0.532000\n",
      "|2023-12-11|21:45:53.921| [INFO] EPOCH: 011\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.72it/s]\n",
      "|2023-12-11|21:45:54.316| [INFO] train-EMA-loss: 0.571179\n",
      "|2023-12-11|21:45:54.684| [INFO] infer-train-accuracy: 0.635000\n",
      "|2023-12-11|21:45:54.992| [INFO] infer-valid-A-accuracy: 0.558000\n",
      "|2023-12-11|21:45:55.278| [INFO] EPOCH: 012\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.98it/s]\n",
      "|2023-12-11|21:45:55.666| [INFO] train-EMA-loss: 0.543728\n",
      "|2023-12-11|21:45:56.025| [INFO] infer-train-accuracy: 0.635000\n",
      "|2023-12-11|21:45:56.337| [INFO] infer-valid-A-accuracy: 0.532000\n",
      "|2023-12-11|21:45:56.622| [INFO] EPOCH: 013\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.09it/s]\n",
      "|2023-12-11|21:45:57.038| [INFO] train-EMA-loss: 0.557633\n",
      "|2023-12-11|21:45:57.412| [INFO] infer-train-accuracy: 0.635000\n",
      "|2023-12-11|21:45:57.732| [INFO] infer-valid-A-accuracy: 0.532000\n",
      "|2023-12-11|21:45:58.032| [INFO] EPOCH: 014\n",
      "100%|██████████| 5/5 [00:00<00:00, 11.72it/s]\n",
      "|2023-12-11|21:45:58.461| [INFO] train-EMA-loss: 0.637043\n",
      "|2023-12-11|21:45:58.861| [INFO] infer-train-accuracy: 0.669000\n",
      "|2023-12-11|21:45:59.181| [INFO] infer-valid-A-accuracy: 0.519000\n",
      "|2023-12-11|21:45:59.508| [INFO] EPOCH: 015\n",
      "100%|██████████| 5/5 [00:00<00:00, 10.56it/s]\n",
      "|2023-12-11|21:45:59.983| [INFO] train-EMA-loss: 0.528854\n",
      "|2023-12-11|21:46:00.365| [INFO] infer-train-accuracy: 0.669000\n",
      "|2023-12-11|21:46:00.667| [INFO] infer-valid-A-accuracy: 0.558000\n",
      "|2023-12-11|21:46:01.005| [INFO] EPOCH: 016\n",
      "100%|██████████| 5/5 [00:00<00:00, 11.74it/s]\n",
      "|2023-12-11|21:46:01.433| [INFO] train-EMA-loss: 0.503937\n",
      "|2023-12-11|21:46:01.851| [INFO] infer-train-accuracy: 0.640000\n",
      "|2023-12-11|21:46:02.233| [INFO] infer-valid-A-accuracy: 0.519000\n",
      "|2023-12-11|21:46:02.566| [INFO] EPOCH: 017\n",
      "100%|██████████| 5/5 [00:00<00:00, 10.48it/s]\n",
      "|2023-12-11|21:46:03.046| [INFO] train-EMA-loss: 0.546567\n",
      "|2023-12-11|21:46:03.447| [INFO] infer-train-accuracy: 0.685000\n",
      "|2023-12-11|21:46:03.778| [INFO] infer-valid-A-accuracy: 0.519000\n",
      "|2023-12-11|21:46:04.075| [INFO] EPOCH: 018\n",
      "100%|██████████| 5/5 [00:00<00:00, 10.60it/s]\n",
      "|2023-12-11|21:46:04.549| [INFO] train-EMA-loss: 0.551418\n",
      "|2023-12-11|21:46:04.970| [INFO] infer-train-accuracy: 0.702000\n",
      "|2023-12-11|21:46:05.344| [INFO] infer-valid-A-accuracy: 0.494000\n",
      "|2023-12-11|21:46:05.644| [INFO] EPOCH: 019\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.29it/s]\n",
      "|2023-12-11|21:46:06.053| [INFO] train-EMA-loss: 0.527870\n",
      "|2023-12-11|21:46:06.447| [INFO] infer-train-accuracy: 0.708000\n",
      "|2023-12-11|21:46:06.761| [INFO] infer-valid-A-accuracy: 0.494000\n",
      "|2023-12-11|21:46:07.059| [INFO] EPOCH: 020\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.50it/s]\n",
      "|2023-12-11|21:46:07.461| [INFO] train-EMA-loss: 0.432746\n",
      "|2023-12-11|21:46:07.878| [INFO] infer-train-accuracy: 0.702000\n",
      "|2023-12-11|21:46:08.197| [INFO] infer-valid-A-accuracy: 0.494000\n",
      "|2023-12-11|21:46:08.483| [INFO] EPOCH: 021\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.32it/s]\n",
      "|2023-12-11|21:46:08.892| [INFO] train-EMA-loss: 0.482441\n",
      "|2023-12-11|21:46:09.334| [INFO] infer-train-accuracy: 0.713000\n",
      "|2023-12-11|21:46:09.708| [INFO] infer-valid-A-accuracy: 0.494000\n",
      "|2023-12-11|21:46:09.999| [INFO] EPOCH: 022\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.59it/s]\n",
      "|2023-12-11|21:46:10.399| [INFO] train-EMA-loss: 0.426708\n",
      "|2023-12-11|21:46:10.803| [INFO] infer-train-accuracy: 0.719000\n",
      "|2023-12-11|21:46:11.127| [INFO] infer-valid-A-accuracy: 0.545000\n",
      "|2023-12-11|21:46:11.416| [INFO] EPOCH: 023\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.73it/s]\n",
      "|2023-12-11|21:46:11.812| [INFO] train-EMA-loss: 0.471340\n",
      "|2023-12-11|21:46:12.209| [INFO] infer-train-accuracy: 0.713000\n",
      "|2023-12-11|21:46:12.525| [INFO] infer-valid-A-accuracy: 0.519000\n",
      "|2023-12-11|21:46:12.814| [INFO] EPOCH: 024\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.45it/s]\n",
      "|2023-12-11|21:46:13.218| [INFO] train-EMA-loss: 0.386153\n",
      "|2023-12-11|21:46:13.614| [INFO] infer-train-accuracy: 0.719000\n",
      "|2023-12-11|21:46:13.960| [INFO] infer-valid-A-accuracy: 0.597000\n",
      "|2023-12-11|21:46:14.257| [INFO] EPOCH: 025\n",
      "100%|██████████| 5/5 [00:00<00:00, 11.96it/s]\n",
      "|2023-12-11|21:46:14.677| [INFO] train-EMA-loss: 0.532177\n",
      "|2023-12-11|21:46:15.074| [INFO] infer-train-accuracy: 0.725000\n",
      "|2023-12-11|21:46:15.414| [INFO] infer-valid-A-accuracy: 0.558000\n",
      "|2023-12-11|21:46:15.766| [INFO] EPOCH: 026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score {'infer-train-accuracy': 0.725, 'infer-valid-A-accuracy': 0.636, 'infer-valid-B-accuracy': 0, 'infer-train-best_epoch': 25, 'infer-valid-A-best_epoch': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 10.65it/s]\n",
      "|2023-12-11|21:46:16.240| [INFO] train-EMA-loss: 0.538121\n",
      "|2023-12-11|21:46:16.651| [INFO] infer-train-accuracy: 0.713000\n",
      "|2023-12-11|21:46:16.981| [INFO] infer-valid-A-accuracy: 0.532000\n",
      "|2023-12-11|21:46:17.280| [INFO] EPOCH: 027\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.70it/s]\n",
      "|2023-12-11|21:46:17.676| [INFO] train-EMA-loss: 0.451188\n",
      "|2023-12-11|21:46:18.077| [INFO] infer-train-accuracy: 0.742000\n",
      "|2023-12-11|21:46:18.416| [INFO] infer-valid-A-accuracy: 0.558000\n",
      "|2023-12-11|21:46:18.713| [INFO] EPOCH: 028\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.59it/s]\n",
      "|2023-12-11|21:46:19.112| [INFO] train-EMA-loss: 0.442870\n",
      "|2023-12-11|21:46:19.504| [INFO] infer-train-accuracy: 0.691000\n",
      "|2023-12-11|21:46:19.832| [INFO] infer-valid-A-accuracy: 0.519000\n",
      "|2023-12-11|21:46:20.136| [INFO] EPOCH: 029\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.00it/s]\n",
      "|2023-12-11|21:46:20.555| [INFO] train-EMA-loss: 0.391929\n",
      "|2023-12-11|21:46:20.948| [INFO] infer-train-accuracy: 0.669000\n",
      "|2023-12-11|21:46:21.290| [INFO] infer-valid-A-accuracy: 0.545000\n",
      "|2023-12-11|21:46:21.609| [INFO] EPOCH: 030\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.07it/s]\n",
      "|2023-12-11|21:46:22.025| [INFO] train-EMA-loss: 0.411091\n",
      "|2023-12-11|21:46:22.445| [INFO] infer-train-accuracy: 0.685000\n",
      "|2023-12-11|21:46:22.777| [INFO] infer-valid-A-accuracy: 0.532000\n",
      "|2023-12-11|21:46:23.081| [INFO] EPOCH: 031\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.16it/s]\n",
      "|2023-12-11|21:46:23.495| [INFO] train-EMA-loss: 0.339823\n",
      "|2023-12-11|21:46:23.942| [INFO] infer-train-accuracy: 0.713000\n",
      "|2023-12-11|21:46:24.311| [INFO] infer-valid-A-accuracy: 0.558000\n",
      "|2023-12-11|21:46:24.615| [INFO] EPOCH: 032\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.54it/s]\n",
      "|2023-12-11|21:46:25.017| [INFO] train-EMA-loss: 0.340630\n",
      "|2023-12-11|21:46:25.398| [INFO] infer-train-accuracy: 0.685000\n",
      "|2023-12-11|21:46:25.716| [INFO] infer-valid-A-accuracy: 0.545000\n",
      "|2023-12-11|21:46:26.017| [INFO] EPOCH: 033\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.99it/s]\n",
      "|2023-12-11|21:46:26.404| [INFO] train-EMA-loss: 0.510508\n",
      "|2023-12-11|21:46:26.780| [INFO] infer-train-accuracy: 0.697000\n",
      "|2023-12-11|21:46:27.102| [INFO] infer-valid-A-accuracy: 0.532000\n",
      "|2023-12-11|21:46:27.396| [INFO] EPOCH: 034\n",
      "100%|██████████| 5/5 [00:00<00:00, 13.07it/s]\n",
      "|2023-12-11|21:46:27.781| [INFO] train-EMA-loss: 0.449442\n",
      "|2023-12-11|21:46:28.177| [INFO] infer-train-accuracy: 0.680000\n",
      "|2023-12-11|21:46:28.507| [INFO] infer-valid-A-accuracy: 0.506000\n",
      "|2023-12-11|21:46:28.810| [INFO] EPOCH: 035\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.77it/s]\n",
      "|2023-12-11|21:46:29.203| [INFO] train-EMA-loss: 0.430597\n",
      "|2023-12-11|21:46:29.602| [INFO] infer-train-accuracy: 0.680000\n",
      "|2023-12-11|21:46:29.928| [INFO] infer-valid-A-accuracy: 0.519000\n",
      "|2023-12-11|21:46:30.235| [INFO] EPOCH: 036\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.58it/s]\n",
      "|2023-12-11|21:46:30.635| [INFO] train-EMA-loss: 0.327289\n",
      "|2023-12-11|21:46:31.022| [INFO] infer-train-accuracy: 0.697000\n",
      "|2023-12-11|21:46:31.349| [INFO] infer-valid-A-accuracy: 0.519000\n",
      "|2023-12-11|21:46:31.653| [INFO] EPOCH: 037\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.04it/s]\n",
      "|2023-12-11|21:46:32.071| [INFO] train-EMA-loss: 0.279150\n",
      "|2023-12-11|21:46:32.496| [INFO] infer-train-accuracy: 0.691000\n",
      "|2023-12-11|21:46:32.860| [INFO] infer-valid-A-accuracy: 0.494000\n",
      "|2023-12-11|21:46:33.163| [INFO] EPOCH: 038\n",
      "100%|██████████| 5/5 [00:00<00:00, 13.05it/s]\n",
      "|2023-12-11|21:46:33.548| [INFO] train-EMA-loss: 0.285457\n",
      "|2023-12-11|21:46:33.931| [INFO] infer-train-accuracy: 0.652000\n",
      "|2023-12-11|21:46:34.255| [INFO] infer-valid-A-accuracy: 0.519000\n",
      "|2023-12-11|21:46:34.556| [INFO] EPOCH: 039\n",
      "100%|██████████| 5/5 [00:00<00:00, 13.41it/s]\n",
      "|2023-12-11|21:46:34.932| [INFO] train-EMA-loss: 0.466609\n",
      "|2023-12-11|21:46:35.299| [INFO] infer-train-accuracy: 0.730000\n",
      "|2023-12-11|21:46:35.633| [INFO] infer-valid-A-accuracy: 0.506000\n",
      "|2023-12-11|21:46:35.942| [INFO] EPOCH: 040\n",
      "100%|██████████| 5/5 [00:00<00:00, 13.05it/s]\n",
      "|2023-12-11|21:46:36.327| [INFO] train-EMA-loss: 0.340792\n",
      "|2023-12-11|21:46:36.700| [INFO] infer-train-accuracy: 0.601000\n",
      "|2023-12-11|21:46:37.024| [INFO] infer-valid-A-accuracy: 0.519000\n",
      "|2023-12-11|21:46:37.335| [INFO] EPOCH: 041\n",
      "100%|██████████| 5/5 [00:00<00:00, 13.03it/s]\n",
      "|2023-12-11|21:46:37.721| [INFO] train-EMA-loss: 0.437380\n",
      "|2023-12-11|21:46:38.094| [INFO] infer-train-accuracy: 0.742000\n",
      "|2023-12-11|21:46:38.432| [INFO] infer-valid-A-accuracy: 0.532000\n",
      "|2023-12-11|21:46:38.724| [INFO] EPOCH: 042\n",
      "100%|██████████| 5/5 [00:00<00:00, 13.10it/s]\n",
      "|2023-12-11|21:46:39.107| [INFO] train-EMA-loss: 0.369540\n",
      "|2023-12-11|21:46:39.493| [INFO] infer-train-accuracy: 0.708000\n",
      "|2023-12-11|21:46:39.865| [INFO] infer-valid-A-accuracy: 0.519000\n",
      "|2023-12-11|21:46:40.171| [INFO] EPOCH: 043\n",
      "100%|██████████| 5/5 [00:00<00:00, 11.19it/s]\n",
      "|2023-12-11|21:46:40.621| [INFO] train-EMA-loss: 0.330399\n",
      "|2023-12-11|21:46:41.015| [INFO] infer-train-accuracy: 0.719000\n",
      "|2023-12-11|21:46:41.354| [INFO] infer-valid-A-accuracy: 0.519000\n",
      "|2023-12-11|21:46:41.645| [INFO] EPOCH: 044\n",
      "100%|██████████| 5/5 [00:00<00:00, 13.09it/s]\n",
      "|2023-12-11|21:46:42.030| [INFO] train-EMA-loss: 0.387980\n",
      "|2023-12-11|21:46:42.394| [INFO] infer-train-accuracy: 0.713000\n",
      "|2023-12-11|21:46:42.728| [INFO] infer-valid-A-accuracy: 0.519000\n",
      "|2023-12-11|21:46:43.023| [INFO] EPOCH: 045\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.07it/s]\n",
      "|2023-12-11|21:46:43.440| [INFO] train-EMA-loss: 0.252186\n",
      "|2023-12-11|21:46:43.823| [INFO] infer-train-accuracy: 0.747000\n",
      "|2023-12-11|21:46:44.139| [INFO] infer-valid-A-accuracy: 0.506000\n",
      "|2023-12-11|21:46:44.433| [INFO] EPOCH: 046\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.85it/s]\n",
      "|2023-12-11|21:46:44.825| [INFO] train-EMA-loss: 0.261386\n",
      "|2023-12-11|21:46:45.217| [INFO] infer-train-accuracy: 0.803000\n",
      "|2023-12-11|21:46:45.554| [INFO] infer-valid-A-accuracy: 0.506000\n",
      "|2023-12-11|21:46:45.850| [INFO] EPOCH: 047\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.48it/s]\n",
      "|2023-12-11|21:46:46.253| [INFO] train-EMA-loss: 0.194541\n",
      "|2023-12-11|21:46:46.652| [INFO] infer-train-accuracy: 0.702000\n",
      "|2023-12-11|21:46:46.970| [INFO] infer-valid-A-accuracy: 0.532000\n",
      "|2023-12-11|21:46:47.275| [INFO] EPOCH: 048\n",
      "100%|██████████| 5/5 [00:00<00:00, 11.00it/s]\n",
      "|2023-12-11|21:46:47.732| [INFO] train-EMA-loss: 0.213928\n",
      "|2023-12-11|21:46:48.213| [INFO] infer-train-accuracy: 0.685000\n",
      "|2023-12-11|21:46:48.597| [INFO] infer-valid-A-accuracy: 0.506000\n",
      "|2023-12-11|21:46:48.943| [INFO] EPOCH: 049\n",
      "100%|██████████| 5/5 [00:00<00:00, 10.40it/s]\n",
      "|2023-12-11|21:46:49.426| [INFO] train-EMA-loss: 0.347770\n",
      "|2023-12-11|21:46:49.822| [INFO] infer-train-accuracy: 0.770000\n",
      "|2023-12-11|21:46:50.176| [INFO] infer-valid-A-accuracy: 0.494000\n",
      "|2023-12-11|21:46:50.475| [INFO] EPOCH: 050\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.43it/s]\n",
      "|2023-12-11|21:46:50.880| [INFO] train-EMA-loss: 0.340659\n",
      "|2023-12-11|21:46:51.276| [INFO] infer-train-accuracy: 0.691000\n",
      "|2023-12-11|21:46:51.614| [INFO] infer-valid-A-accuracy: 0.506000\n",
      "|2023-12-11|21:46:51.933| [INFO] EPOCH: 051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score {'infer-train-accuracy': 0.803, 'infer-valid-A-accuracy': 0.636, 'infer-valid-B-accuracy': 0, 'infer-train-best_epoch': 46, 'infer-valid-A-best_epoch': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 11.83it/s]\n",
      "|2023-12-11|21:46:52.358| [INFO] train-EMA-loss: 0.197974\n",
      "|2023-12-11|21:46:52.741| [INFO] infer-train-accuracy: 0.781000\n",
      "|2023-12-11|21:46:53.094| [INFO] infer-valid-A-accuracy: 0.506000\n",
      "|2023-12-11|21:46:53.408| [INFO] EPOCH: 052\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.05it/s]\n",
      "|2023-12-11|21:46:53.825| [INFO] train-EMA-loss: 0.180647\n",
      "|2023-12-11|21:46:54.263| [INFO] infer-train-accuracy: 0.792000\n",
      "|2023-12-11|21:46:54.585| [INFO] infer-valid-A-accuracy: 0.494000\n",
      "|2023-12-11|21:46:54.934| [INFO] EPOCH: 053\n",
      "100%|██████████| 5/5 [00:00<00:00, 11.88it/s]\n",
      "|2023-12-11|21:46:55.357| [INFO] train-EMA-loss: 0.185316\n",
      "|2023-12-11|21:46:55.737| [INFO] infer-train-accuracy: 0.680000\n",
      "|2023-12-11|21:46:56.077| [INFO] infer-valid-A-accuracy: 0.545000\n",
      "|2023-12-11|21:46:56.427| [INFO] EPOCH: 054\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.99it/s]\n",
      "|2023-12-11|21:46:56.816| [INFO] train-EMA-loss: 0.160134\n",
      "|2023-12-11|21:46:57.174| [INFO] infer-train-accuracy: 0.702000\n",
      "|2023-12-11|21:46:57.473| [INFO] infer-valid-A-accuracy: 0.532000\n",
      "|2023-12-11|21:46:57.786| [INFO] EPOCH: 055\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.58it/s]\n",
      "|2023-12-11|21:46:58.186| [INFO] train-EMA-loss: 0.247973\n",
      "|2023-12-11|21:46:58.593| [INFO] infer-train-accuracy: 0.781000\n",
      "|2023-12-11|21:46:58.924| [INFO] infer-valid-A-accuracy: 0.506000\n",
      "|2023-12-11|21:46:59.237| [INFO] EPOCH: 056\n",
      "100%|██████████| 5/5 [00:00<00:00, 11.28it/s]\n",
      "|2023-12-11|21:46:59.683| [INFO] train-EMA-loss: 0.121074\n",
      "|2023-12-11|21:47:00.073| [INFO] infer-train-accuracy: 0.893000\n",
      "|2023-12-11|21:47:00.411| [INFO] infer-valid-A-accuracy: 0.494000\n",
      "|2023-12-11|21:47:00.737| [INFO] EPOCH: 057\n",
      "100%|██████████| 5/5 [00:00<00:00, 10.00it/s]\n",
      "|2023-12-11|21:47:01.239| [INFO] train-EMA-loss: 0.264101\n",
      "|2023-12-11|21:47:01.670| [INFO] infer-train-accuracy: 0.837000\n",
      "|2023-12-11|21:47:01.991| [INFO] infer-valid-A-accuracy: 0.532000\n",
      "|2023-12-11|21:47:02.302| [INFO] EPOCH: 058\n",
      "100%|██████████| 5/5 [00:00<00:00, 14.13it/s]\n",
      "|2023-12-11|21:47:02.658| [INFO] train-EMA-loss: 0.123519\n",
      "|2023-12-11|21:47:03.003| [INFO] infer-train-accuracy: 0.674000\n",
      "|2023-12-11|21:47:03.346| [INFO] infer-valid-A-accuracy: 0.532000\n",
      "|2023-12-11|21:47:03.691| [INFO] EPOCH: 059\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.51it/s]\n",
      "|2023-12-11|21:47:04.092| [INFO] train-EMA-loss: 0.256976\n",
      "|2023-12-11|21:47:04.457| [INFO] infer-train-accuracy: 0.725000\n",
      "|2023-12-11|21:47:04.792| [INFO] infer-valid-A-accuracy: 0.532000\n",
      "|2023-12-11|21:47:05.112| [INFO] EPOCH: 060\n",
      "100%|██████████| 5/5 [00:00<00:00, 13.85it/s]\n",
      "|2023-12-11|21:47:05.475| [INFO] train-EMA-loss: 0.129104\n",
      "|2023-12-11|21:47:05.839| [INFO] infer-train-accuracy: 0.803000\n",
      "|2023-12-11|21:47:06.138| [INFO] infer-valid-A-accuracy: 0.506000\n",
      "|2023-12-11|21:47:06.502| [INFO] EPOCH: 061\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.88it/s]\n",
      "|2023-12-11|21:47:06.892| [INFO] train-EMA-loss: 0.117323\n",
      "|2023-12-11|21:47:07.301| [INFO] infer-train-accuracy: 0.876000\n",
      "|2023-12-11|21:47:07.684| [INFO] infer-valid-A-accuracy: 0.481000\n",
      "|2023-12-11|21:47:08.014| [INFO] EPOCH: 062\n",
      "100%|██████████| 5/5 [00:00<00:00, 14.42it/s]\n",
      "|2023-12-11|21:47:08.363| [INFO] train-EMA-loss: 0.139662\n",
      "|2023-12-11|21:47:08.712| [INFO] infer-train-accuracy: 0.713000\n",
      "|2023-12-11|21:47:09.019| [INFO] infer-valid-A-accuracy: 0.506000\n",
      "|2023-12-11|21:47:09.338| [INFO] EPOCH: 063\n",
      "100%|██████████| 5/5 [00:00<00:00, 13.43it/s]\n",
      "|2023-12-11|21:47:09.712| [INFO] train-EMA-loss: 0.110106\n",
      "|2023-12-11|21:47:10.085| [INFO] infer-train-accuracy: 0.730000\n",
      "|2023-12-11|21:47:10.397| [INFO] infer-valid-A-accuracy: 0.494000\n",
      "|2023-12-11|21:47:10.715| [INFO] EPOCH: 064\n",
      "100%|██████████| 5/5 [00:00<00:00, 14.69it/s]\n",
      "|2023-12-11|21:47:11.058| [INFO] train-EMA-loss: 0.092600\n",
      "|2023-12-11|21:47:11.400| [INFO] infer-train-accuracy: 0.843000\n",
      "|2023-12-11|21:47:11.720| [INFO] infer-valid-A-accuracy: 0.519000\n",
      "|2023-12-11|21:47:12.049| [INFO] EPOCH: 065\n",
      "100%|██████████| 5/5 [00:00<00:00, 14.70it/s]\n",
      "|2023-12-11|21:47:12.392| [INFO] train-EMA-loss: 0.171438\n",
      "|2023-12-11|21:47:12.780| [INFO] infer-train-accuracy: 0.865000\n",
      "|2023-12-11|21:47:13.078| [INFO] infer-valid-A-accuracy: 0.481000\n",
      "|2023-12-11|21:47:13.385| [INFO] EPOCH: 066\n",
      "100%|██████████| 5/5 [00:00<00:00, 14.35it/s]\n",
      "|2023-12-11|21:47:13.735| [INFO] train-EMA-loss: 0.081936\n",
      "|2023-12-11|21:47:14.091| [INFO] infer-train-accuracy: 0.742000\n",
      "|2023-12-11|21:47:14.399| [INFO] infer-valid-A-accuracy: 0.506000\n",
      "|2023-12-11|21:47:14.716| [INFO] EPOCH: 067\n",
      "100%|██████████| 5/5 [00:00<00:00, 14.12it/s]\n",
      "|2023-12-11|21:47:15.072| [INFO] train-EMA-loss: 0.111504\n",
      "|2023-12-11|21:47:15.426| [INFO] infer-train-accuracy: 0.708000\n",
      "|2023-12-11|21:47:15.724| [INFO] infer-valid-A-accuracy: 0.519000\n",
      "|2023-12-11|21:47:16.052| [INFO] EPOCH: 068\n",
      "100%|██████████| 5/5 [00:00<00:00, 13.94it/s]\n",
      "|2023-12-11|21:47:16.413| [INFO] train-EMA-loss: 0.127617\n",
      "|2023-12-11|21:47:16.799| [INFO] infer-train-accuracy: 0.826000\n",
      "|2023-12-11|21:47:17.113| [INFO] infer-valid-A-accuracy: 0.532000\n",
      "|2023-12-11|21:47:17.429| [INFO] EPOCH: 069\n",
      "100%|██████████| 5/5 [00:00<00:00, 14.27it/s]\n",
      "|2023-12-11|21:47:17.781| [INFO] train-EMA-loss: 0.132238\n",
      "|2023-12-11|21:47:18.131| [INFO] infer-train-accuracy: 0.876000\n",
      "|2023-12-11|21:47:18.432| [INFO] infer-valid-A-accuracy: 0.532000\n",
      "|2023-12-11|21:47:18.741| [INFO] EPOCH: 070\n",
      "100%|██████████| 5/5 [00:00<00:00, 14.13it/s]\n",
      "|2023-12-11|21:47:19.097| [INFO] train-EMA-loss: 0.115874\n",
      "|2023-12-11|21:47:19.490| [INFO] infer-train-accuracy: 0.820000\n",
      "|2023-12-11|21:47:19.819| [INFO] infer-valid-A-accuracy: 0.519000\n",
      "|2023-12-11|21:47:20.143| [INFO] EPOCH: 071\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.49it/s]\n",
      "|2023-12-11|21:47:20.547| [INFO] train-EMA-loss: 0.104555\n",
      "|2023-12-11|21:47:20.969| [INFO] infer-train-accuracy: 0.826000\n",
      "|2023-12-11|21:47:21.332| [INFO] infer-valid-A-accuracy: 0.558000\n",
      "|2023-12-11|21:47:21.679| [INFO] EPOCH: 072\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.58it/s]\n",
      "|2023-12-11|21:47:22.080| [INFO] train-EMA-loss: 0.082457\n",
      "|2023-12-11|21:47:22.476| [INFO] infer-train-accuracy: 0.882000\n",
      "|2023-12-11|21:47:22.823| [INFO] infer-valid-A-accuracy: 0.532000\n",
      "|2023-12-11|21:47:23.165| [INFO] EPOCH: 073\n",
      "100%|██████████| 5/5 [00:00<00:00, 11.46it/s]\n",
      "|2023-12-11|21:47:23.603| [INFO] train-EMA-loss: 0.096083\n",
      "|2023-12-11|21:47:24.017| [INFO] infer-train-accuracy: 0.882000\n",
      "|2023-12-11|21:47:24.374| [INFO] infer-valid-A-accuracy: 0.506000\n",
      "|2023-12-11|21:47:24.774| [INFO] EPOCH: 074\n",
      "100%|██████████| 5/5 [00:00<00:00, 11.72it/s]\n",
      "|2023-12-11|21:47:25.203| [INFO] train-EMA-loss: 0.087129\n",
      "|2023-12-11|21:47:25.592| [INFO] infer-train-accuracy: 0.843000\n",
      "|2023-12-11|21:47:25.936| [INFO] infer-valid-A-accuracy: 0.532000\n",
      "|2023-12-11|21:47:26.269| [INFO] EPOCH: 075\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.73it/s]\n",
      "|2023-12-11|21:47:26.664| [INFO] train-EMA-loss: 0.177791\n",
      "|2023-12-11|21:47:27.079| [INFO] infer-train-accuracy: 0.893000\n",
      "|2023-12-11|21:47:27.416| [INFO] infer-valid-A-accuracy: 0.532000\n",
      "|2023-12-11|21:47:27.749| [INFO] EPOCH: 076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score {'infer-train-accuracy': 0.893, 'infer-valid-A-accuracy': 0.636, 'infer-valid-B-accuracy': 0, 'infer-train-best_epoch': 75, 'infer-valid-A-best_epoch': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 13.03it/s]\n",
      "|2023-12-11|21:47:28.135| [INFO] train-EMA-loss: 0.204511\n",
      "|2023-12-11|21:47:28.511| [INFO] infer-train-accuracy: 0.921000\n",
      "|2023-12-11|21:47:28.882| [INFO] infer-valid-A-accuracy: 0.506000\n",
      "|2023-12-11|21:47:29.217| [INFO] EPOCH: 077\n",
      "100%|██████████| 5/5 [00:00<00:00, 11.47it/s]\n",
      "|2023-12-11|21:47:29.655| [INFO] train-EMA-loss: 0.062215\n",
      "|2023-12-11|21:47:30.112| [INFO] infer-train-accuracy: 0.843000\n",
      "|2023-12-11|21:47:30.487| [INFO] infer-valid-A-accuracy: 0.558000\n",
      "|2023-12-11|21:47:30.819| [INFO] EPOCH: 078\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.28it/s]\n",
      "|2023-12-11|21:47:31.228| [INFO] train-EMA-loss: 0.075478\n",
      "|2023-12-11|21:47:31.655| [INFO] infer-train-accuracy: 0.921000\n",
      "|2023-12-11|21:47:31.996| [INFO] infer-valid-A-accuracy: 0.519000\n",
      "|2023-12-11|21:47:32.306| [INFO] EPOCH: 079\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.29it/s]\n",
      "|2023-12-11|21:47:32.715| [INFO] train-EMA-loss: 0.159792\n",
      "|2023-12-11|21:47:33.125| [INFO] infer-train-accuracy: 0.843000\n",
      "|2023-12-11|21:47:33.485| [INFO] infer-valid-A-accuracy: 0.506000\n",
      "|2023-12-11|21:47:33.858| [INFO] EPOCH: 080\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.46it/s]\n",
      "|2023-12-11|21:47:34.262| [INFO] train-EMA-loss: 0.204762\n",
      "|2023-12-11|21:47:34.659| [INFO] infer-train-accuracy: 0.966000\n",
      "|2023-12-11|21:47:35.008| [INFO] infer-valid-A-accuracy: 0.481000\n",
      "|2023-12-11|21:47:35.387| [INFO] EPOCH: 081\n",
      "100%|██████████| 5/5 [00:00<00:00, 10.97it/s]\n",
      "|2023-12-11|21:47:35.846| [INFO] train-EMA-loss: 0.138742\n",
      "|2023-12-11|21:47:37.157| [INFO] infer-train-accuracy: 0.865000\n",
      "|2023-12-11|21:47:37.504| [INFO] infer-valid-A-accuracy: 0.455000\n",
      "|2023-12-11|21:47:37.847| [INFO] EPOCH: 082\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.39it/s]\n",
      "|2023-12-11|21:47:38.253| [INFO] train-EMA-loss: 0.179211\n",
      "|2023-12-11|21:47:38.670| [INFO] infer-train-accuracy: 0.843000\n",
      "|2023-12-11|21:47:38.996| [INFO] infer-valid-A-accuracy: 0.506000\n",
      "|2023-12-11|21:47:39.296| [INFO] EPOCH: 083\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.57it/s]\n",
      "|2023-12-11|21:47:39.696| [INFO] train-EMA-loss: 0.088886\n",
      "|2023-12-11|21:47:40.125| [INFO] infer-train-accuracy: 0.893000\n",
      "|2023-12-11|21:47:40.462| [INFO] infer-valid-A-accuracy: 0.506000\n",
      "|2023-12-11|21:47:40.764| [INFO] EPOCH: 084\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.01it/s]\n",
      "|2023-12-11|21:47:41.183| [INFO] train-EMA-loss: 0.088678\n",
      "|2023-12-11|21:47:41.569| [INFO] infer-train-accuracy: 0.888000\n",
      "|2023-12-11|21:47:41.897| [INFO] infer-valid-A-accuracy: 0.429000\n",
      "|2023-12-11|21:47:42.200| [INFO] EPOCH: 085\n",
      "100%|██████████| 5/5 [00:00<00:00, 11.55it/s]\n",
      "|2023-12-11|21:47:42.635| [INFO] train-EMA-loss: 0.080866\n",
      "|2023-12-11|21:47:43.008| [INFO] infer-train-accuracy: 0.876000\n",
      "|2023-12-11|21:47:43.336| [INFO] infer-valid-A-accuracy: 0.532000\n",
      "|2023-12-11|21:47:43.646| [INFO] EPOCH: 086\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.02it/s]\n",
      "|2023-12-11|21:47:44.064| [INFO] train-EMA-loss: 0.077966\n",
      "|2023-12-11|21:47:44.444| [INFO] infer-train-accuracy: 0.809000\n",
      "|2023-12-11|21:47:44.768| [INFO] infer-valid-A-accuracy: 0.545000\n",
      "|2023-12-11|21:47:45.072| [INFO] EPOCH: 087\n",
      "100%|██████████| 5/5 [00:00<00:00, 11.05it/s]\n",
      "|2023-12-11|21:47:45.527| [INFO] train-EMA-loss: 0.085725\n",
      "|2023-12-11|21:47:45.907| [INFO] infer-train-accuracy: 0.787000\n",
      "|2023-12-11|21:47:46.230| [INFO] infer-valid-A-accuracy: 0.519000\n",
      "|2023-12-11|21:47:46.559| [INFO] EPOCH: 088\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.96it/s]\n",
      "|2023-12-11|21:47:46.948| [INFO] train-EMA-loss: 0.054480\n",
      "|2023-12-11|21:47:47.330| [INFO] infer-train-accuracy: 0.876000\n",
      "|2023-12-11|21:47:47.652| [INFO] infer-valid-A-accuracy: 0.519000\n",
      "|2023-12-11|21:47:47.954| [INFO] EPOCH: 089\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.88it/s]\n",
      "|2023-12-11|21:47:48.344| [INFO] train-EMA-loss: 0.088512\n",
      "|2023-12-11|21:47:48.740| [INFO] infer-train-accuracy: 0.972000\n",
      "|2023-12-11|21:47:49.093| [INFO] infer-valid-A-accuracy: 0.494000\n",
      "|2023-12-11|21:47:49.410| [INFO] EPOCH: 090\n",
      "100%|██████████| 5/5 [00:00<00:00, 11.99it/s]\n",
      "|2023-12-11|21:47:49.829| [INFO] train-EMA-loss: 0.097727\n",
      "|2023-12-11|21:47:50.211| [INFO] infer-train-accuracy: 0.955000\n",
      "|2023-12-11|21:47:50.528| [INFO] infer-valid-A-accuracy: 0.481000\n",
      "|2023-12-11|21:47:50.827| [INFO] EPOCH: 091\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.52it/s]\n",
      "|2023-12-11|21:47:51.228| [INFO] train-EMA-loss: 0.099212\n",
      "|2023-12-11|21:47:51.605| [INFO] infer-train-accuracy: 0.921000\n",
      "|2023-12-11|21:47:51.933| [INFO] infer-valid-A-accuracy: 0.481000\n",
      "|2023-12-11|21:47:52.250| [INFO] EPOCH: 092\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.41it/s]\n",
      "|2023-12-11|21:47:52.655| [INFO] train-EMA-loss: 0.049644\n",
      "|2023-12-11|21:47:53.050| [INFO] infer-train-accuracy: 0.826000\n",
      "|2023-12-11|21:47:53.428| [INFO] infer-valid-A-accuracy: 0.506000\n",
      "|2023-12-11|21:47:53.726| [INFO] EPOCH: 093\n",
      "100%|██████████| 5/5 [00:00<00:00, 11.84it/s]\n",
      "|2023-12-11|21:47:54.150| [INFO] train-EMA-loss: 0.077941\n",
      "|2023-12-11|21:47:54.542| [INFO] infer-train-accuracy: 0.854000\n",
      "|2023-12-11|21:47:54.864| [INFO] infer-valid-A-accuracy: 0.519000\n",
      "|2023-12-11|21:47:55.168| [INFO] EPOCH: 094\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.68it/s]\n",
      "|2023-12-11|21:47:55.564| [INFO] train-EMA-loss: 0.051965\n",
      "|2023-12-11|21:47:55.936| [INFO] infer-train-accuracy: 0.944000\n",
      "|2023-12-11|21:47:56.259| [INFO] infer-valid-A-accuracy: 0.519000\n",
      "|2023-12-11|21:47:56.561| [INFO] EPOCH: 095\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.13it/s]\n",
      "|2023-12-11|21:47:56.976| [INFO] train-EMA-loss: 0.080311\n",
      "|2023-12-11|21:47:57.359| [INFO] infer-train-accuracy: 0.961000\n",
      "|2023-12-11|21:47:57.688| [INFO] infer-valid-A-accuracy: 0.506000\n",
      "|2023-12-11|21:47:58.010| [INFO] EPOCH: 096\n",
      "100%|██████████| 5/5 [00:00<00:00, 11.84it/s]\n",
      "|2023-12-11|21:47:58.435| [INFO] train-EMA-loss: 0.044295\n",
      "|2023-12-11|21:47:58.833| [INFO] infer-train-accuracy: 0.966000\n",
      "|2023-12-11|21:47:59.145| [INFO] infer-valid-A-accuracy: 0.506000\n",
      "|2023-12-11|21:47:59.453| [INFO] EPOCH: 097\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.35it/s]\n",
      "|2023-12-11|21:47:59.860| [INFO] train-EMA-loss: 0.044305\n",
      "|2023-12-11|21:48:00.239| [INFO] infer-train-accuracy: 0.972000\n",
      "|2023-12-11|21:48:00.581| [INFO] infer-valid-A-accuracy: 0.506000\n",
      "|2023-12-11|21:48:00.888| [INFO] EPOCH: 098\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.23it/s]\n",
      "|2023-12-11|21:48:01.299| [INFO] train-EMA-loss: 0.039834\n",
      "|2023-12-11|21:48:01.702| [INFO] infer-train-accuracy: 0.904000\n",
      "|2023-12-11|21:48:02.030| [INFO] infer-valid-A-accuracy: 0.506000\n",
      "|2023-12-11|21:48:02.335| [INFO] EPOCH: 099\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.76it/s]\n",
      "|2023-12-11|21:48:02.729| [INFO] train-EMA-loss: 0.079062\n",
      "|2023-12-11|21:48:03.104| [INFO] infer-train-accuracy: 0.949000\n",
      "|2023-12-11|21:48:03.419| [INFO] infer-valid-A-accuracy: 0.519000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'infer-train-accuracy': 0.972, 'infer-valid-A-accuracy': 0.636, 'infer-valid-B-accuracy': 0, 'infer-train-best_epoch': 97, 'infer-valid-A-best_epoch': 2}\n"
     ]
    }
   ],
   "source": [
    "# # we must define the function after training/loading\n",
    "# def nodes_preproc_func(node_features: np.ndarray) -> np.ndarray:\n",
    "#     \"\"\"Pre-processing function for nodes.\"\"\"\n",
    "#     return node_scaler.transform(node_features)\n",
    "nodes_preproc_func = None\n",
    "\n",
    "\n",
    "splits = joblib.load(SPLIT_PATH)\n",
    "loader_kwargs = {\n",
    "    \"num_workers\": 6,\n",
    "    \"batch_size\": 32,\n",
    "}\n",
    "# arch_kwargs = {\n",
    "#     \"dim_features\": NUM_NODE_FEATURES,\n",
    "#     \"dim_target\": NCLASSES,\n",
    "#     \"layers\": [32, 32, 16, 8],\n",
    "#     \"dropout\": 0.3,\n",
    "#     \"pooling\": \"mean\",\n",
    "#     \"conv\": \"EdgeConv\",\n",
    "#     \"aggr\": \"max\",\n",
    "# }\n",
    "\n",
    "if LABEL_TYPE == \"OS\":\n",
    "    NCLASSES = 2\n",
    "else:\n",
    "    NCLASSES = 6\n",
    "    \n",
    "conv = \"EdgeConv\"\n",
    "pooling = \"mean\"\n",
    "aggr = \"max\"\n",
    "dropout= 0.15\n",
    "layers = [64, 32, 32]\n",
    "\n",
    "arch_kwargs = {\n",
    "        \"dim_features\": NUM_NODE_FEATURES,\n",
    "        \"dim_target\": NCLASSES,\n",
    "        \"layers\": layers,\n",
    "        \"dropout\": dropout,\n",
    "        \"pooling\": pooling,\n",
    "        \"conv\": conv,\n",
    "        \"aggr\": aggr,\n",
    "        \"CLASSIFICATION_TYPE\" : LABEL_TYPE\n",
    "}\n",
    "\n",
    "RUN_OUTPUT_DIR = TRAIN_DIR / f\"session_{STAIN}_{conv}_{pooling}_{aggr}_{str(dropout)}_{str(layers)}_{datetime.now().strftime('%m_%d_%H_%M')}\"\n",
    "RUN_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_DIR = RUN_OUTPUT_DIR / \"model\"\n",
    "\n",
    "optim_kwargs = {\n",
    "    \"lr\": 5.0e-4,\n",
    "    \"weight_decay\": 1.0e-4,\n",
    "}\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "# if not MODEL_DIR.exists() or True:\n",
    "for split_idx, split in enumerate(splits):\n",
    "        new_split = {\n",
    "            \"train\": split[\"train\"],\n",
    "            \"infer-train\": split[\"train\"],\n",
    "            \"infer-valid-A\": split[\"valid\"],\n",
    "            # \"infer-valid-B\": split[\"test\"],\n",
    "        }\n",
    "        MODEL_DIR = Path(MODEL_DIR) \n",
    "        split_save_dir = MODEL_DIR / f\"{split_idx:02d}/\"\n",
    "        rm_n_mkdir(split_save_dir)\n",
    "        reset_logging(split_save_dir)\n",
    "        output = run_once(new_split,\n",
    "            NUM_EPOCHS,\n",
    "            save_dir=split_save_dir,\n",
    "            arch_kwargs=arch_kwargs,\n",
    "            loader_kwargs=loader_kwargs,\n",
    "            optim_kwargs=optim_kwargs,\n",
    "            on_gpu=ON_GPU,\n",
    "            GRAPH_DIR=GRAPHSDIR,\n",
    "            LABEL_TYPE = LABEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds  surv_event\n",
      "0      False         18\n",
      "       True          20\n",
      "1      False         31\n",
      "       True           8\n",
      "Name: gt, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gt</th>\n",
       "      <th>surv_event</th>\n",
       "      <th>surv_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preds</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>4.417632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>9.741026</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        gt  surv_event  surv_time\n",
       "preds                            \n",
       "0      0.0    0.526316   4.417632\n",
       "1      1.0    0.205128   9.741026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds  surv_event\n",
      "0      False          6\n",
      "       True           8\n",
      "1      False         43\n",
      "       True          20\n",
      "Name: gt, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gt</th>\n",
       "      <th>surv_event</th>\n",
       "      <th>surv_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preds</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>5.45500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.587302</td>\n",
       "      <td>0.317460</td>\n",
       "      <td>7.48254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             gt  surv_event  surv_time\n",
       "preds                                 \n",
       "0      0.142857    0.571429    5.45500\n",
       "1      0.587302    0.317460    7.48254"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sksurv.nonparametric import kaplan_meier_estimator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_km_curve_222(df, save_name= \"test1.png\"):\n",
    "\n",
    "    plt.figure()\n",
    "    color_dict = {1: 'b',\n",
    "                0: 'r',\n",
    "                2: 'y'}\n",
    "    \n",
    "    treatment_type_name = {0: 'High',\n",
    "                1: 'Low',\n",
    "                2: 'y'}\n",
    "\n",
    "    for treatment_type in (0,1):\n",
    "\n",
    "        color = color_dict[treatment_type]\n",
    "        mask_treat = df[f\"preds\"] == treatment_type\n",
    "        time_treatment, survival_prob_treatment, conf_int = kaplan_meier_estimator(\n",
    "            df[f\"surv_event\"][mask_treat],\n",
    "            df[f\"surv_time\"][mask_treat],\n",
    "            conf_type=\"log-log\",\n",
    "        )\n",
    "        \n",
    "        plt.step(time_treatment, survival_prob_treatment, where=\"post\", \n",
    "                 label=f\"Risk group = {treatment_type_name[treatment_type]}\", color=color)\n",
    "        plt.fill_between(time_treatment, conf_int[0], conf_int[1], alpha=0.1, step=\"post\", color=color)\n",
    "\n",
    "        # print(time_treatment)\n",
    "\n",
    "    plt.ylim(0, 1)\n",
    "    plt.ylabel(\"Survival\")\n",
    "    plt.xlabel(\"time $t$\")\n",
    "    plt.title(\"KM curve on Test set\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_name)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def get_km_plot_multi(output):\n",
    "    preds= output[0][:,0]\n",
    "    gt= output[1][:,0]\n",
    "\n",
    "    median_value = np.mean(preds)\n",
    "    print(np.mean(preds), np.mean(preds), np.median(preds), np.mean(preds))\n",
    "    preds[preds >= median_value] = 1\n",
    "    preds[preds < median_value] = 0\n",
    "\n",
    "    output_df =pd.DataFrame([preds, gt, output[2], output[3]]).T\n",
    "    output_df.columns = (\"preds\", \"gt\", \"surv_event\" ,\"surv_time\")\n",
    "\n",
    "    output_df['surv_event'] = output_df['surv_event'].astype(bool)\n",
    "    output_df['preds'] = output_df['preds'].astype(int)\n",
    "    output_df['surv_event'] = output_df['surv_event'].astype(bool)\n",
    "\n",
    "    # plot_km_curve_222(output_df)\n",
    "\n",
    "    return output_df\n",
    "\n",
    "\n",
    "def get_km_plot(output):\n",
    "    # preds= output[0][:,0]\n",
    "    # gt= output[1][:,0]\n",
    "    orig_preds = output[-1]['infer-valid-A-pred']\n",
    "    gt = output[-1]['infer-valid-A-gt']\n",
    "    surv_time_list = output[-1]['infer-valid-A-surv_time_list']\n",
    "    surv_event_list = output[-1]['infer-valid-A-surv_event_list']\n",
    "    \n",
    "    # print(\"orig_preds\", preds)\n",
    "    # print(\"gt\", gt)\n",
    "    # print(\"surv_time_list\", surv_time_list)\n",
    "    # print(\"surv_event_list\", surv_event_list)\n",
    "    # quant_1 = np.quantile(preds, 0.33)\n",
    "    # quant_2 = np.quantile(preds, 0.66)\n",
    "    # print(quant_1, quant_2, np.mean(preds), np.mean(preds), np.median(preds), np.mean(preds))\n",
    "    # orig_preds[ (preds < quant_1) ] = 0\n",
    "    # orig_preds[ (preds >= quant_1) * (preds <= quant_2) ] = 1\n",
    "    # orig_preds[ (preds > quant_2) ] = 2\n",
    "\n",
    "    preds = gt.copy()\n",
    "    output_df =pd.DataFrame([preds, gt, surv_event_list, surv_time_list]).T\n",
    "    output_df.columns = (\"preds\", \"gt\", \"surv_event\" ,\"surv_time\")\n",
    "    output_df['surv_event'] = output_df['surv_event'].astype(bool)\n",
    "    output_df['preds'] = output_df['preds'].astype(int)\n",
    "    print(output_df.groupby(['preds', 'surv_event']).count()['gt'] )\n",
    "    display(output_df.groupby(['preds']).mean())\n",
    "    plot_km_curve_222(output_df,  save_name= \"train.png\")\n",
    "    \n",
    "    preds = orig_preds.argmax(axis=1)\n",
    "    output_df =pd.DataFrame([preds, gt, surv_event_list, surv_time_list]).T\n",
    "    output_df.columns = (\"preds\", \"gt\", \"surv_event\" ,\"surv_time\")\n",
    "    output_df['surv_event'] = output_df['surv_event'].astype(bool)\n",
    "    output_df['preds'] = output_df['preds'].astype(int)\n",
    "    print(output_df.groupby(['preds', 'surv_event']).count()['gt'] )\n",
    "    display(output_df.groupby(['preds']).mean())\n",
    "    plot_km_curve_222(output_df, save_name= \"test.png\")\n",
    "\n",
    "    return output_df\n",
    "\n",
    "output_df = get_km_plot(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = [[1.46648765e+00,-8.77613187e-01,-2.51502216e-01,-1.10481453e+00\n",
    ",6.64071560e-01,-5.99045038e-01]\n",
    ",[-1.80002856e+00,8.66671383e-01,-6.64208949e-01,1.20571077e+00\n",
    ",-2.28768730e+00,1.23205519e+00]\n",
    ",[-8.38510573e-01,2.11250722e-01,4.82147932e-03,5.65632761e-01\n",
    ",-1.94593978e+00,6.11969411e-01]\n",
    ",[1.70954013e+00,-7.06104040e-01,-6.74402267e-02,-8.06845903e-01\n",
    ",1.92917514e+00,-1.58036017e+00]\n",
    ",[1.23513281e+00,-6.98642850e-01,-7.90928781e-01,-6.50385141e-01\n",
    ",9.64580655e-01,-5.73552132e-01]\n",
    ",[-9.15723860e-01,3.66611242e-01,3.33443969e-01,4.22663540e-01\n",
    ",-7.69037247e-01,1.99665681e-01]]\n",
    "\n",
    "gt = [[1.,1.,0.,0.,0.,0.]\n",
    ",[0.,0.,0.,0.,0.,1.]\n",
    ",[1.,1.,0.,1.,1.,0.]\n",
    ",[1.,0.,0.,0.,0.,0.]\n",
    ",[0.,1.,1.,0.,0.,1.]\n",
    ",[1.,1.,1.,1.,1.,0.]]\n",
    "\n",
    "\n",
    "def multilabel_accuracy(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Calculate multilabel classification metrics.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true: 2D array-like or binary indicator matrix, true labels\n",
    "    - y_pred: 2D array-like or binary indicator matrix, predicted labels\n",
    "\n",
    "    Returns:\n",
    "    - overall_accuracy: Subset accuracy (exact match ratio)\n",
    "    - label_accuracies: List of accuracies for each individual label\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_pred[y_pred > 0.5] = 1\n",
    "    y_pred[y_pred < 0.5] = 0\n",
    "\n",
    "    # Ensure the shapes are the same\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(\"Shapes of y_true and y_pred must be the same.\")\n",
    "    \n",
    "    label_accuracies = np.mean(y_true == y_pred, axis=0).round(4)\n",
    "    overall_accuracy = np.mean(label_accuracies).round(4)\n",
    "\n",
    "    return overall_accuracy, label_accuracies\n",
    "\n",
    "multilabel_accuracy(pred, gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
