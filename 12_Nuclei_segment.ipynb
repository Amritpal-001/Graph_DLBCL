{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTfV-nd683PZ"
      },
      "source": [
        "# Nucleus Instance Segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCOSzUCUXnfh",
        "tags": [
          "remove-cell"
        ]
      },
      "outputs": [],
      "source": [
        "# %%bash\n",
        "# apt-get -y install libopenjp2-7-dev libopenjp2-tools openslide-tools libpixman-1-dev | tail -n 1\n",
        "# pip install git+https://github.com/TissueImageAnalytics/tiatoolbox.git@develop | tail -n 1\n",
        "# echo \"Installation is done.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UEIfjUTaJLPj"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "|2023-11-04|12:19:38.582| [WARNING] /home/amrit/anaconda3/envs/DL/lib/python3.10/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
            "  @numba.jit()\n",
            "\n",
            "|2023-11-04|12:19:38.583| [WARNING] /home/amrit/anaconda3/envs/DL/lib/python3.10/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
            "  @numba.jit()\n",
            "\n",
            "|2023-11-04|12:19:38.584| [WARNING] /home/amrit/anaconda3/envs/DL/lib/python3.10/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
            "  @numba.jit()\n",
            "\n",
            "|2023-11-04|12:19:38.952| [WARNING] /home/amrit/anaconda3/envs/DL/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "\n",
            "|2023-11-04|12:19:41.710| [WARNING] /home/amrit/anaconda3/envs/DL/lib/python3.10/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
            "  @numba.jit()\n",
            "\n",
            "|2023-11-04|12:19:42.649| [WARNING] /tmp/ipykernel_10397/1192300010.py:34: DeprecationWarning: This function has no longer any effect, and will be removed in a future release. Starting with Shapely 2.0, equivalent speedups are always available\n",
            "  speedups.enable()\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\"\"\"Import modules required to run the Jupyter notebook.\"\"\"\n",
        "# Clear logger to use tiatoolbox.logger\n",
        "import logging\n",
        "import warnings\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "if logging.getLogger().hasHandlers():\n",
        "    logging.getLogger().handlers.clear()\n",
        "\n",
        "from tiatoolbox.data import stain_norm_target\n",
        "from tiatoolbox.tools.stainnorm import get_normalizer\n",
        "import shutil\n",
        "import slideio\n",
        "\n",
        "import cv2\n",
        "import joblib\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "from matplotlib import colormaps\n",
        "from numpy.typing import ArrayLike\n",
        "from PIL import Image, ImageFilter, ImageOps\n",
        "from shapely import speedups\n",
        "from shapely.geometry import Polygon\n",
        "\n",
        "from tiatoolbox import logger\n",
        "from tiatoolbox.annotation.storage import Annotation, AnnotationStore\n",
        "\n",
        "if speedups.available:  # pragma: no branch\n",
        "    speedups.enable()\n",
        "\n",
        "\n",
        "from tiatoolbox import logger\n",
        "from tiatoolbox.models.engine.nucleus_instance_segmentor import NucleusInstanceSegmentor\n",
        "from tiatoolbox.utils.misc import download_data, imread\n",
        "\n",
        "# We need this function to visualize the nuclear predictions\n",
        "from tiatoolbox.utils.visualization import (\n",
        "    overlay_prediction_contours,\n",
        ")\n",
        "from tiatoolbox.wsicore.wsireader import WSIReader\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "mpl.rcParams[\"figure.dpi\"] = 300  # for high resolution figure in notebook\n",
        "mpl.rcParams[\"figure.facecolor\"] = \"white\"  # To make sure text is visible in dark mode\n",
        "plt.rcParams.update({\"font.size\": 5})\n",
        "\n",
        "from src.visualize import overlay_prediction_contours"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> ![image](https://tiatoolbox.dcs.warwick.ac.uk/notebook/hovernet_samples.PNG)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df2= pd.read_csv(\"/home/amrit/data/proj_data/MLG_project/DLBCL-Morph/TMA/annotations_clean.csv\")\n",
        "print(df2['stain'].unique())\n",
        "# df2 = df2[df2['patient_id'] == 17666]\n",
        "# df2 = df2[df2['patient_id'] == 13968]\n",
        "# df2 = df2[df2['patient_id'] == 17658]\n",
        "\n",
        "print(df2['stain'].unique())\n",
        "print(df2['tma_id'].unique())\n",
        "\n",
        "\n",
        "df2 = df2.sort_values(\"patient_id\")\n",
        "# df2 = df2[df2['tma_id'] == 'TA292']\n",
        "df2['area'] = (df2['xe'] - df2['xs']) *  (df2['ye'] - df2['ys'])/10000\n",
        "df2 = df2[df2['area'] >= 150]  \n",
        "\n",
        "df2 = df2[df2['stain'] == 'MYC']\n",
        "df2 = df2[df2['xs']  >=0 ]\n",
        "df2 = df2[df2['ys']  >=0 ]\n",
        "df2 = df2[df2['xe']  >=0 ]\n",
        "df2 = df2[df2['ye']  >=0 ]\n",
        "\n",
        "df2 = df2.reset_index()\n",
        "\n",
        "\n",
        "df2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df2.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df2[240:260]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Single run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "ON_GPU= True\n",
        "patch_size = 224\n",
        "output_size = patch_size*8\n",
        "\n",
        "# index = np.random.choice(df2.index)\n",
        "\n",
        "import glob\n",
        "# glob.glob(f\"/home/amrit/data/proj_data/MLG_project/DLBCL-Morph/outputs/images/**/*HE**.png\", recursive=True)\n",
        "\n",
        "start_index = 10\n",
        "end_index = len(df2.index)\n",
        "\n",
        "\n",
        "#PanNuke Dataset - H/E   'hovernet_fast-pannuke'   #19 different tissue types.  481 visual fields, 20K WSI at different magnifications, data sources https://paperswithcode.com/dataset/pannuke\n",
        "#CoNSeP Dataset -   'micronet_hovernet-consep'  41 H&E stained image tile   https://paperswithcode.com/dataset/consep\n",
        "#MoNuSAC Dataset - H/E   'hovernet_fast-monusac'  #  4 different organs (Lung, Prostate, Kidney, and Breast)  #https://monusac-2020.grand-challenge.org/Data/\n",
        "#Kumar Dataset- H&E staining    'hovernet_original_kumar'  #https://monuseg.grand-challenge.org/Data/\n",
        "\n",
        "\n",
        "model_name = \"hovernet_fast-pannuke\" #: #, \"hovernet_fast-monusac\"]\n",
        "# Tile prediction\n",
        "inst_segmentor = NucleusInstanceSegmentor(\n",
        "    pretrained_model=model_name , #\",  #hovernet_fast-pannuke\", hovernet_fast-monusac\n",
        "    num_loader_workers=4,\n",
        "    num_postproc_workers=4,\n",
        "    batch_size=8,\n",
        ")\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Create an empty list to store information for failed cases\n",
        "failed_cases = []\n",
        "\n",
        "for index in range(start_index,end_index) : #len(df2.index)):\n",
        "    try:\n",
        "        # index = np.random.choice(df2.index)\n",
        "        # index = 1626\n",
        "\n",
        "        print( \"index\", index)\n",
        "        print(df2.iloc[index])\n",
        "\n",
        "\n",
        "        df_index = df2['index'][index]\n",
        "        patient_id = df2['patient_id'][index]\n",
        "        stain = df2['stain'][index]\n",
        "        tma_id = df2['tma_id'][index]\n",
        "        xs\t,ys\t,xe\t,ye = df2[['xs'\t,'ys'\t,'xe'\t,'ye']].iloc[index].to_list()\n",
        "\n",
        "        print(patient_id, stain, tma_id, xs, ys, xe, ye, xe-xs, ye-ys)\n",
        "\n",
        "        svs_path = f\"/home/amrit/data/proj_data/MLG_project/DLBCL-Morph/TMA/{stain}/{tma_id}.svs\"\n",
        "        save_dir = f\"/home/amrit/data/proj_data/MLG_project/DLBCL-Morph/outputs/files/{stain}/{patient_id}/{df_index}/\"\n",
        "        parent_folder= f\"/home/amrit/data/proj_data/MLG_project/DLBCL-Morph/outputs/images/{patient_id}/\"\n",
        "\n",
        "        import os\n",
        "        try:\n",
        "            os.mkdir(parent_folder)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        img_file_name = f\"{parent_folder}{patient_id}_{stain}_{tma_id}_{output_size}_{df_index}.png\"\n",
        "        kmeans_cluster_vis_file = f\"{parent_folder}{patient_id}_{stain}_{tma_id}_{output_size}_{df_index}_kmeans_cluster_vis.png\"\n",
        "        overlay_image_file_name = f\"{parent_folder}{patient_id}_{stain}_{tma_id}_{output_size}_{df_index}_overlay.npy\"\n",
        "        overlay_mask_file_name = f\"{parent_folder}{patient_id}_{stain}_{tma_id}_{output_size}_{df_index}_mask.npy\"\n",
        "        overlay_vis_file_name =  f\"{parent_folder}{patient_id}_{stain}_{tma_id}_{output_size}_{df_index}_vis.png\"\n",
        "\n",
        "        # slide = slideio.open_slide(svs_path,'SVS')\n",
        "        # scene = slide.get_scene(0)\n",
        "        # print(index , slide.num_scenes , slide.raw_metadata)\n",
        "\n",
        "        reader = WSIReader.open(svs_path)\n",
        "        info_dict = reader.info.as_dict()\n",
        "        print(info_dict)  # noqa: T203\n",
        "\n",
        "        buffer = 150\n",
        "        # Specify the bounds in terms of rectangle (left, top, right, bottom)\n",
        "        bounds = (xs - buffer,ys - buffer\t,xe  + buffer\t,ye + buffer)\n",
        "\n",
        "        # Read the region using wsi reader's read bounds at level 0\n",
        "        img_array = reader.read_bounds(bounds, resolution=0, units=\"level\")\n",
        "        print(img_array.shape)\n",
        "        # plt.imshow(img_array)\n",
        "        # plt.axis(\"off\")\n",
        "        # plt.show()\n",
        "\n",
        "        image_pil = Image.fromarray(img_array)\n",
        "        image_pil.save(img_file_name)\n",
        "\n",
        "        # plot_index = 1\n",
        "        # for x_index in range(0,img_array.shape[0],150):\n",
        "        #     plt.subplot(5,5, plot_index)\n",
        "        #     plt.imshow(img_array[x_index:x_index+patch_size,x_index:x_index+patch_size])\n",
        "        #     plot_index+= 1\n",
        "        # plt.show()\n",
        "\n",
        "\n",
        "        try:\n",
        "            shutil.rmtree(save_dir)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        inst_segmentor.ioconfig.tile_shape = [output_size,output_size]\n",
        "        # inst_segmentor.ioconfig.patch_input_shape = [patch_size,patch_size]\n",
        "        # inst_segmentor.ioconfig.patch_output_shape = [patch_size,patch_size]\n",
        "        # inst_segmentor.ioconfig.margin = [patch_size,patch_size]\n",
        "\n",
        "        # inst_segmentor.ioconfig.highest_input_resolution = {'units': 'mpp', 'resolution': 0.1}\n",
        "\n",
        "        # inst_segmentor.ioconfig.patch_input_shape = [224,224]\n",
        "\n",
        "        # stain_norm_method = 'ruifrok'\n",
        "        stain_norm_method = \"None\"\n",
        "\n",
        "        if stain_norm_method != \"None\":\n",
        "            target_image = stain_norm_target()\n",
        "            # \"reinhard\", \"custom\", \"ruifrok\", \"macenko\" or \"vahadane\".\n",
        "            stain_normalizer = get_normalizer(stain_norm_method) \n",
        "            stain_normalizer.fit(target_image)\n",
        "\n",
        "            def stain_norm_func(img: np.ndarray) -> np.ndarray:\n",
        "                \"\"\"Helper function to perform stain normalization.\"\"\"\n",
        "                return stain_normalizer.transform(img)\n",
        "\n",
        "            inst_segmentor.model.preproc_func = stain_norm_func\n",
        "\n",
        "        tile_output = inst_segmentor.predict(\n",
        "            [img_file_name],\n",
        "            save_dir= save_dir,\n",
        "            mode=\"tile\",\n",
        "            on_gpu=ON_GPU,\n",
        "            crash_on_exception=True,\n",
        "        )\n",
        "\n",
        "        tile_preds = joblib.load(f\"{tile_output[0][1]}.dat\")\n",
        "        logger.info(f\"Number of detected nuclei: {len(tile_preds)}\")\n",
        "\n",
        "        # import json \n",
        "            \n",
        "        # with open(json_file_name, \"w\") as outfile: \n",
        "        # \tjson.dump(tile_preds, outfile)\n",
        "            \n",
        "        # Extracting the nucleus IDs and select the first one\n",
        "        nuc_id_list = list(tile_preds.keys())\n",
        "        selected_nuc_id = nuc_id_list[0]\n",
        "        logger.info(f\"Nucleus prediction structure for nucleus ID: {selected_nuc_id}\")\n",
        "        sample_nuc = tile_preds[selected_nuc_id]\n",
        "        sample_nuc_keys = list(sample_nuc)\n",
        "        logger.info(\n",
        "            \"Keys in the output dictionary: [%s, %s, %s, %s, %s]\",\n",
        "            sample_nuc_keys[0],\n",
        "            sample_nuc_keys[1],\n",
        "            sample_nuc_keys[2],\n",
        "            sample_nuc_keys[3],\n",
        "            sample_nuc_keys[4],\n",
        "        )\n",
        "        logger.info(\n",
        "            \"Bounding box: (%d, %d, %d, %d)\",\n",
        "            sample_nuc[\"box\"][0],\n",
        "            sample_nuc[\"box\"][1],\n",
        "            sample_nuc[\"box\"][2],\n",
        "            sample_nuc[\"box\"][3],\n",
        "        )\n",
        "        logger.info(\n",
        "            \"Centroid: (%d, %d)\",\n",
        "            sample_nuc[\"centroid\"][0],\n",
        "            sample_nuc[\"centroid\"][1],\n",
        "        )\n",
        "\n",
        "        tile_img = imread(img_file_name)\n",
        "\n",
        "        color_dict = {\n",
        "            0: (\"neoplastic epithelial\", (255, 0, 0)),\n",
        "            1: (\"Inflammatory\", (255, 255, 0)),\n",
        "            2: (\"Connective\", (0, 255, 0)),\n",
        "            3: (\"Dead\", (0, 0, 0)),\n",
        "            4: (\"non-neoplastic epithelial\", (0, 0, 255)),\n",
        "        }\n",
        "\n",
        "\n",
        "        # Filter dictionary cell types\n",
        "        count_dict = {0: 0,\n",
        "                    1:0,\n",
        "                    2:0,\n",
        "                    3:0,\n",
        "                    4:0,\n",
        "                    5:0}\n",
        "        selected_count_dict = {0: 0,\n",
        "                    1:0,\n",
        "                    2:0,\n",
        "                    3:0,\n",
        "                    4:0,\n",
        "                    5:0}\n",
        "\n",
        "        cell_prob_list = []\n",
        "        new_tile_preds = {}\n",
        "        for x in tile_preds:\n",
        "            cell_type = tile_preds[x]['type']\n",
        "            cell_prob = tile_preds[x]['prob']\n",
        "            cell_prob_list.append(cell_prob)\n",
        "\n",
        "            count_dict[cell_type] += 1\n",
        "            if cell_prob > 0.5: #cell_type != 0 and \n",
        "                new_tile_preds[x] = tile_preds[x]\n",
        "                selected_count_dict[cell_type] += 1\n",
        "\n",
        "\n",
        "        print(\"count_dict\" , count_dict)\n",
        "        print(\"selected_count_dict\" , selected_count_dict)\n",
        "\n",
        "        overlay_mask, overlay_image = overlay_prediction_contours(\n",
        "            canvas=tile_img,\n",
        "            inst_dict=tile_preds,\n",
        "            draw_dot=False,\n",
        "            type_colours=color_dict,\n",
        "            line_thickness=2,\n",
        "        )\n",
        "\n",
        "        # overlay_mask_selected, overlay_image_selected  = overlay_prediction_contours(\n",
        "        #     canvas=tile_img,\n",
        "        #     inst_dict=new_tile_preds,\n",
        "        #     draw_dot=False,\n",
        "        #     type_colours=color_dict,\n",
        "        #     line_thickness=2,\n",
        "        # )\n",
        "\n",
        "        # showing processed results alongside the original images\n",
        "        fig = plt.figure(figsize=(15,6))\n",
        "        ax1 = plt.subplot(1, 3, 1), plt.imshow(tile_img), plt.axis(\"off\")\n",
        "        ax3 = plt.subplot(1, 3, 2), plt.imshow(overlay_mask), plt.axis(\"off\")\n",
        "        ax2 = plt.subplot(1, 3, 3), plt.imshow(overlay_image), plt.axis(\"off\")\n",
        "        # ax3 = plt.subplot(1, 3, 3), plt.imshow(overlay_mask_selected), plt.axis(\"off\")\n",
        "\n",
        "        plt.title(f\"Stain_norm_method_{stain_norm_method}\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(overlay_vis_file_name, dpi=200)\n",
        "        # plt.show()\n",
        "        plt.clf()\n",
        "\n",
        "        # np.save( overlay_image_file_name, overlay_image)\n",
        "        np.save(overlay_mask_file_name, overlay_mask)\n",
        "\n",
        "    except Exception as e:\n",
        "\n",
        "        failed_cases.append({'file_name': img_file_name, 'index': index, 'df2_index': df_index, 'patient_id': patient_id,\n",
        "                            \"stain\": stain, \"tma_id\": tma_id})\n",
        "        failed_df = pd.DataFrame(failed_cases)\n",
        "        failed_df.to_csv('failed_cases.csv', index=False)\n",
        "\n",
        "        print(\"***********************************************\")\n",
        "        print(\"***********************************************\")\n",
        "        print(\"***********************************************\")\n",
        "        print(\"***********************************************\")\n",
        "        print(f\"AAAAA Failed case: {index} \")\n",
        "        print(f\"AAAAA Failed case: {index} \")\n",
        "        print(f\"AAAAA Failed case: {index} \")\n",
        "        print(f\"AAAAA Failed case: {index} \")\n",
        "\n",
        "        print(\"***********************************************\")\n",
        "        print(\"***********************************************\")\n",
        "        print(\"***********************************************\")\n",
        "        print(\"***********************************************\")\n",
        "        print(\"***********************************************\")\n",
        "        print(\"***********************************************\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "celltoolbar": "Raw Cell Format",
    "colab": {
      "name": "08-nucleus-instance-segmentation.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "641317031b9f54126a387462bfafcc8a3ed3bbe28fae551a718f7a4af76cf9e8"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
