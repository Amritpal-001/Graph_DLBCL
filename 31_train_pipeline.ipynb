{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "from os import path as osp\n",
    "\n",
    "import torch\n",
    "from tiatoolbox.utils.misc import select_device\n",
    "import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import json\n",
    "import glob\n",
    "\n",
    "# from src.intensity import add_features_and_create_new_dicts\n",
    "\n",
    "from src.featureextraction import get_cell_features, add_features_and_create_new_dicts\n",
    "from src.train import stratified_split, recur_find_ext, run_once, rm_n_mkdir ,reset_logging\n",
    "from src.graph_construct import create_graph_with_pooled_patch_nodes, get_pids_labels_for_key\n",
    "\n",
    "\n",
    "ON_GPU = False\n",
    "device = select_device(on_gpu=ON_GPU)\n",
    "\n",
    "SEED = 5\n",
    "random.seed(SEED)\n",
    "rng = np.random.default_rng(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "\n",
    "\n",
    "BASEDIR = '/home/amrit/data/proj_data/MLG_project/DLBCL-Morph'\n",
    "\n",
    "\n",
    "# STAIN = 'MYC'\n",
    "# STAIN = 'BCL2'\n",
    "STAIN = 'HE'\n",
    "\n",
    "FIDIR = f'{BASEDIR}/outputs'\n",
    "CLINPATH = f'{BASEDIR}/clinical_data_cleaned.csv'\n",
    "ANNPATH = f'{BASEDIR}/annotations_clean.csv'\n",
    "FEATSDIR = f'{BASEDIR}/outputs/files/{STAIN}'\n",
    "FEATSCALERPATH = f\"{FEATSDIR}/0_feat_scaler.npz\"\n",
    "PATCH_SIZE = 224\n",
    "OUTPUT_SIZE = PATCH_SIZE*8\n",
    "\n",
    "WORKSPACE_DIR = Path(BASEDIR)\n",
    "# GRAPH_DIR = WORKSPACE_DIR / f\"graphs{STAIN}\" \n",
    "# LABELS_PATH = WORKSPACE_DIR / \"graphs/0_labels.txt\"\n",
    "\n",
    "\n",
    "# Graph construction\n",
    "# PATCH_SIZE = 300\n",
    "SKEW_NOISE = 0.0001\n",
    "MIN_CELLS_PER_PATCH = 10\n",
    "CONNECTIVITY_DISTANCE = 500\n",
    "\n",
    "LABEL_TYPE = 'multilabel' #'OS' #\n",
    "\n",
    "GRAPHSDIR = Path(f'{BASEDIR}/graphs/{STAIN}')\n",
    "LABELSPATH = f'{BASEDIR}/graphs/{STAIN}_labels.json'\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "NUM_NODE_FEATURES = 128\n",
    "NCLASSES = 3\n",
    "\n",
    "TRAIN_DIR = WORKSPACE_DIR / \"training\"\n",
    "SPLIT_PATH = TRAIN_DIR / f\"splits_{STAIN}.dat\"\n",
    "RUN_OUTPUT_DIR = TRAIN_DIR / f\"session_{STAIN}_{datetime.now().strftime('%m_%d_%H_%M')}\"\n",
    "RUN_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_DIR = RUN_OUTPUT_DIR / \"model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract features: 20 + 4 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read annotation csv, filter, and process intensity features\n",
    "df = pd.read_csv(ANNPATH)\n",
    "\n",
    "df = df[df['stain'] == STAIN]\n",
    "df['area'] = (df['xe'] - df['xs']) *  (df['ye'] - df['ys'])/10000\n",
    "df = df[df['area'] >= 150]  \n",
    "df = df[df['xs']  >=0 ]\n",
    "df = df[df['ys']  >=0 ]\n",
    "df = df[df['xe']  >=0 ]\n",
    "df = df[df['ye']  >=0 ]\n",
    "\n",
    "df = df.reset_index()\n",
    "##########\n",
    "\n",
    "###############\n",
    "# add intensity features\n",
    "start_index = 0\n",
    "end_index = len(df.index)\n",
    "\n",
    "datpaths = []\n",
    "imgpaths = []\n",
    "updatpaths = []\n",
    "\n",
    "# for index in range(start_index, 1):\n",
    "for index in range(start_index, end_index):\n",
    "    df_index = df['index'][index]\n",
    "    patient_id = df['patient_id'][index]\n",
    "    stain = df['stain'][index]\n",
    "    tma_id = df['tma_id'][index]\n",
    "    unique_id = str(patient_id) + '_' + stain + '_' + str(df_index)\n",
    "\n",
    "    img_file_name = f\"{FIDIR}/images/{patient_id}/{patient_id}_{stain}_{tma_id}_{OUTPUT_SIZE}_{df_index}.png\"\n",
    "    dat_file_name = f\"{FIDIR}/files/{stain}/{patient_id}/{df_index}/0.dat\"\n",
    "    updat_file_name = f\"{FIDIR}/files/{stain}/{patient_id}/{df_index}/{unique_id}.dat\"\n",
    "\n",
    "    datpaths.append(dat_file_name)\n",
    "    imgpaths.append(img_file_name)\n",
    "    updatpaths.append(updat_file_name)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updatpaths[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16 minutes\n",
    "# shutil.rmtree(FEATSDIR)\n",
    "# if not osp.exists(FEATSDIR):\n",
    "#     os.makedirs(FEATSDIR)\n",
    "add_features_and_create_new_dicts(datpaths, imgpaths, updatpaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 minutes\n",
    "gns = StandardScaler()\n",
    "for featpath in tqdm(updatpaths):\n",
    "    try:\n",
    "        celldatadict = joblib.load(featpath)\n",
    "        cellsfeats = np.array([v['intensity_feats'] for k, v in celldatadict.items()])\n",
    "        gns.partial_fit(cellsfeats)\n",
    "    except:\n",
    "        print(featpath)\n",
    "\n",
    "np.savez(FEATSCALERPATH, mean=gns.mean_, var=gns.var_)\n",
    "\n",
    "# dd = np.load(FEATSCALERPATH)\n",
    "# print(dd['mean'], gns.mean_)\n",
    "# print(dd['var'], gns.var_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATSCALERPATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare graphs : 4 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.graph_construct import get_pids_multilabels_for_key\n",
    "\n",
    "df = pd.read_csv(CLINPATH)\n",
    "df =df.fillna(0)\n",
    "# display(df.info())\n",
    "\n",
    "\n",
    "if LABEL_TYPE == 'multilabel':\n",
    "    df_labels = get_pids_multilabels_for_key(df, key_list=['OS','MYC IHC', 'BCL2 IHC', 'BCL6 IHC', 'CD10 IHC', 'MUM1 IHC'], nclasses=2)\n",
    "else:\n",
    "    df_labels = get_pids_labels_for_key(df, key ='OS', nclasses=3)\n",
    "\n",
    "df_labels\n",
    "\n",
    "# Graph construction\n",
    "PATCH_SIZE = 300\n",
    "SKEW_NOISE = 0.0001\n",
    "MIN_CELLS_PER_PATCH = 10\n",
    "CONNECTIVITY_DISTANCE = 500\n",
    "\n",
    "# # save paths\n",
    "# featpaths = np.sort(glob.glob(f'{FEATSDIR}/**/*.dat', recursive=True)) #np.sort(glob.glob(f'{FEATSDIR}/*.dat'))\n",
    "# featpaths = [x if \"/0.dat\" not in x for x in featpaths]\n",
    "featpaths = np.sort(glob.glob(f'{FEATSDIR}/**/*.dat', recursive=True)) #np.sort(glob.glob(f'{FEATSDIR}/*.dat'))\n",
    "featpaths = [x for x in featpaths if (\"/0.dat\" not in x) and (\"/file_map.dat\" not in x)]\n",
    "featpaths\n",
    "pids = [int(osp.basename(featpath).split('_')[0]) for featpath in featpaths]\n",
    "df_featpaths = pd.DataFrame(zip(pids, featpaths), columns=['patient_id', 'featpath'])\n",
    "\n",
    "# merge to find datapoints with graph data and labels\n",
    "df_data = df_featpaths.merge(df_labels, on='patient_id')\n",
    "# df_data = df_data[:12]\n",
    "\n",
    "featpaths_data = df_data['featpath'].to_list()\n",
    "# labels_data = df_data['OS_class'].to_list()\n",
    "labels_data = df_data[['OS_class','MYC IHC_class', 'BCL2 IHC_class', 'BCL6 IHC_class', 'CD10 IHC_class', 'MUM1 IHC_class']].to_numpy().tolist()\n",
    "display(labels_data[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "outgraphpaths_data = [f\"{GRAPHSDIR}/{osp.basename(featpath).split('.')[0]}.json\" for featpath in featpaths_data]\n",
    "\n",
    "# save labels\n",
    "labels_dict = {osp.basename(graphpath): label for graphpath, label in zip(outgraphpaths_data, labels_data)}\n",
    "with open(LABELSPATH, 'w') as f:\n",
    "    json.dump(labels_dict, f)\n",
    "\n",
    "# read normalizer stats from file and pass to fn\n",
    "dd = np.load(FEATSCALERPATH)\n",
    "cell_feat_norm_stats = (dd['mean'], dd['var'])\n",
    "\n",
    "# create final graphs data\n",
    "shutil.rmtree(GRAPHSDIR)\n",
    "if not osp.exists(GRAPHSDIR):\n",
    "    os.makedirs(GRAPHSDIR)\n",
    "    create_graph_with_pooled_patch_nodes(\n",
    "        featpaths_data,\n",
    "        labels_data,\n",
    "        outgraphpaths_data,\n",
    "        PATCH_SIZE,\n",
    "        cell_feat_norm_stats=cell_feat_norm_stats,\n",
    "        MIN_CELLS_PER_PATCH= MIN_CELLS_PER_PATCH,\n",
    "        CONNECTIVITY_DISTANCE = CONNECTIVITY_DISTANCE\n",
    "    )\n",
    "    # wont work in parallel mode\n",
    "    # print(np.mean(global_patch_stats), np.std(global_patch_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsi_paths = recur_find_ext(GRAPHSDIR, [\".json\"])\n",
    "wsi_names = [Path(v).stem for v in wsi_paths]\n",
    "assert len(wsi_paths) > 0, \"No files found.\"  # noqa: S101\n",
    "\n",
    "len(wsi_paths) , len(wsi_names) , wsi_names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dset import multilabel_stratified_split\n",
    "\n",
    "NUM_FOLDS = 1\n",
    "TEST_RATIO = 0.2\n",
    "TRAIN_RATIO = 0.8 * 0.7\n",
    "VALID_RATIO = 0.8 * 0.3\n",
    "\n",
    "# if SPLIT_PATH and os.path.exists(SPLIT_PATH):\n",
    "#     splits = joblib.load(SPLIT_PATH)\n",
    "# else:\n",
    "x = np.array(wsi_names)\n",
    "with open(LABELSPATH, 'r') as f:\n",
    "    labels_dict = json.load(f)\n",
    "print(labels_dict)\n",
    "y = np.array([labels_dict[wsi_name+'.json'] for wsi_name in wsi_names])\n",
    "# y[np.where(y==-1)] = 0\n",
    "if LABEL_TYPE == \"multilabel\":\n",
    "    splits = multilabel_stratified_split(x, y, TRAIN_RATIO, VALID_RATIO, TEST_RATIO, NUM_FOLDS)\n",
    "else:\n",
    "    splits = stratified_split(x, y, TRAIN_RATIO, VALID_RATIO, TEST_RATIO, NUM_FOLDS)\n",
    "\n",
    "joblib.dump(splits, SPLIT_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import load_json, rm_n_mkdir, mkdir, recur_find_ext\n",
    "from src.dset import SlideGraphDataset, stratified_split, StratifiedSampler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "nodes_preproc_func = None\n",
    "\n",
    "def sample_data(  # noqa: C901, PLR0912, PLR0915\n",
    "    dataset_dict: dict,\n",
    "    GRAPH_DIR = None) -> list:\n",
    "    \n",
    "    for subset_name, subset in dataset_dict.items():\n",
    "        ds = SlideGraphDataset(subset, mode=\"train\", preproc=nodes_preproc_func, graph_dir=GRAPH_DIR)\n",
    "\n",
    "        batch_sampler = None\n",
    "\n",
    "        _loader_kwargs = {\n",
    "            \"num_workers\": 6,\n",
    "            \"batch_size\": 32,\n",
    "        }\n",
    "        # # arch_kwargs = {\n",
    "\n",
    "        # print(\"subset\" ,subset_name , len(subset))\n",
    "        \n",
    "        # batch_sampler = batch_sampler = StratifiedSampler(\n",
    "        #             labels=[v[1] for v in subset],\n",
    "        #             batch_size=32,\n",
    "        #         )\n",
    "\n",
    "        loader =  DataLoader(ds,\n",
    "                    batch_sampler=batch_sampler,\n",
    "            drop_last=subset_name == \"train\" and batch_sampler is None,\n",
    "            shuffle=subset_name == \"train\" and batch_sampler is None,\n",
    "            **_loader_kwargs,)\n",
    "        \n",
    "    return ds, loader\n",
    "\n",
    "\n",
    "for split_idx, split in enumerate(splits):\n",
    "    new_split = {\n",
    "                \"train\": split[\"train\"],\n",
    "                \"infer-train\": split[\"train\"],\n",
    "                \"infer-valid-A\": split[\"valid\"],\n",
    "                \"infer-valid-B\": split[\"test\"],\n",
    "            }\n",
    "    ds, loader = sample_data(new_split, GRAPHSDIR)\n",
    "\n",
    "sample = ds.__getitem__(3)\n",
    "display(sample)\n",
    "\n",
    "for _step, batch_data in enumerate(loader):\n",
    "    print(batch_data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.model import SlideGraphArch\n",
    "\n",
    "# arch_kwargs = {\n",
    "#     \"dim_features\": NUM_NODE_FEATURES,\n",
    "#     \"dim_target\": 3,\n",
    "#     \"layers\": [64, 32, 32],\n",
    "#     \"dropout\": 0.3,\n",
    "#     \"pooling\": \"mean\",\n",
    "#     \"conv\": \"EdgeConv\",\n",
    "#     \"aggr\": \"max\",\n",
    "#     \"CLASSIFICATION_TYPE\" : LABEL_TYPE\n",
    "# }\n",
    "\n",
    "# model = SlideGraphArch(**arch_kwargs)\n",
    "\n",
    "# for _step, batch_data in enumerate(tqdm(loader, disable=True)):\n",
    "#     device = select_device(on_gpu=False)\n",
    "#     wsi_graphs = batch_data[\"graph\"].to(device)\n",
    "#     model = model.to(device)\n",
    "\n",
    "#     # Data type conversion\n",
    "#     wsi_graphs.x = wsi_graphs.x.type(torch.float32)\n",
    "\n",
    "#     # Inference mode\n",
    "#     model.eval()\n",
    "#     # Do not compute the gradient (not training)\n",
    "#     with torch.inference_mode():\n",
    "#         wsi_output, _ = model(wsi_graphs)\n",
    "#         # print(\"wsi_output\" ,wsi_output)\n",
    "#     # print(\"xxx\")\n",
    "#     break\n",
    "    \n",
    "# # pred = torch.nn.functional.softmax(wsi_output.type(torch.float32))\n",
    "# # gt = batch_data['label'].type(torch.float32)\n",
    "# # display(pred.shape , gt.shape)\n",
    "# # # criterion = torch.nn.BCELoss()\n",
    "# # # criterion = torch.nn.()\n",
    "\n",
    "# # loss = torch.nn.functional.cross_entropy(pred, gt)\n",
    "# # loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # we must define the function after training/loading\n",
    "# def nodes_preproc_func(node_features: np.ndarray) -> np.ndarray:\n",
    "#     \"\"\"Pre-processing function for nodes.\"\"\"\n",
    "#     return node_scaler.transform(node_features)\n",
    "nodes_preproc_func = None\n",
    "\n",
    "\n",
    "splits = joblib.load(SPLIT_PATH)\n",
    "loader_kwargs = {\n",
    "    \"num_workers\": 6,\n",
    "    \"batch_size\": 32,\n",
    "}\n",
    "# arch_kwargs = {\n",
    "#     \"dim_features\": NUM_NODE_FEATURES,\n",
    "#     \"dim_target\": NCLASSES,\n",
    "#     \"layers\": [32, 32, 16, 8],\n",
    "#     \"dropout\": 0.3,\n",
    "#     \"pooling\": \"mean\",\n",
    "#     \"conv\": \"EdgeConv\",\n",
    "#     \"aggr\": \"max\",\n",
    "# }\n",
    "\n",
    "if LABEL_TYPE == \"OS\":\n",
    "    NCLASSES = 3\n",
    "else:\n",
    "    NCLASSES = 6\n",
    "      \n",
    "arch_kwargs = {\n",
    "        \"dim_features\": NUM_NODE_FEATURES,\n",
    "        \"dim_target\": NCLASSES,\n",
    "        \"layers\": [64, 32, 32],\n",
    "        \"dropout\": 0.3,\n",
    "        \"pooling\": \"mean\",\n",
    "        \"conv\": \"EdgeConv\",\n",
    "        \"aggr\": \"max\",\n",
    "        \"CLASSIFICATION_TYPE\" : LABEL_TYPE\n",
    "}\n",
    "\n",
    "optim_kwargs = {\n",
    "    \"lr\": 5.0e-3,\n",
    "    \"weight_decay\": 1.0e-4,\n",
    "}\n",
    "\n",
    "NUM_EPOCHS = 30\n",
    "# if not MODEL_DIR.exists() or True:\n",
    "for split_idx, split in enumerate(splits):\n",
    "        new_split = {\n",
    "            \"train\": split[\"train\"],\n",
    "            \"infer-train\": split[\"train\"],\n",
    "            \"infer-valid-A\": split[\"valid\"],\n",
    "            \"infer-valid-B\": split[\"test\"],\n",
    "        }\n",
    "        MODEL_DIR = Path(MODEL_DIR) \n",
    "        split_save_dir = MODEL_DIR / f\"{split_idx:02d}/\"\n",
    "        rm_n_mkdir(split_save_dir)\n",
    "        reset_logging(split_save_dir)\n",
    "        run_once(\n",
    "            new_split,\n",
    "            NUM_EPOCHS,\n",
    "            save_dir=split_save_dir,\n",
    "            arch_kwargs=arch_kwargs,\n",
    "            loader_kwargs=loader_kwargs,\n",
    "            optim_kwargs=optim_kwargs,\n",
    "            on_gpu=ON_GPU,\n",
    "            GRAPH_DIR=GRAPHSDIR,\n",
    "            LABEL_TYPE = LABEL_TYPE\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4722, array([0.5   , 0.1667, 0.6667, 0.6667, 0.1667, 0.6667]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = [[1.46648765e+00,-8.77613187e-01,-2.51502216e-01,-1.10481453e+00\n",
    ",6.64071560e-01,-5.99045038e-01]\n",
    ",[-1.80002856e+00,8.66671383e-01,-6.64208949e-01,1.20571077e+00\n",
    ",-2.28768730e+00,1.23205519e+00]\n",
    ",[-8.38510573e-01,2.11250722e-01,4.82147932e-03,5.65632761e-01\n",
    ",-1.94593978e+00,6.11969411e-01]\n",
    ",[1.70954013e+00,-7.06104040e-01,-6.74402267e-02,-8.06845903e-01\n",
    ",1.92917514e+00,-1.58036017e+00]\n",
    ",[1.23513281e+00,-6.98642850e-01,-7.90928781e-01,-6.50385141e-01\n",
    ",9.64580655e-01,-5.73552132e-01]\n",
    ",[-9.15723860e-01,3.66611242e-01,3.33443969e-01,4.22663540e-01\n",
    ",-7.69037247e-01,1.99665681e-01]]\n",
    "\n",
    "gt = [[1.,1.,0.,0.,0.,0.]\n",
    ",[0.,0.,0.,0.,0.,1.]\n",
    ",[1.,1.,0.,1.,1.,0.]\n",
    ",[1.,0.,0.,0.,0.,0.]\n",
    ",[0.,1.,1.,0.,0.,1.]\n",
    ",[1.,1.,1.,1.,1.,0.]]\n",
    "\n",
    "\n",
    "def multilabel_accuracy(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Calculate multilabel classification metrics.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true: 2D array-like or binary indicator matrix, true labels\n",
    "    - y_pred: 2D array-like or binary indicator matrix, predicted labels\n",
    "\n",
    "    Returns:\n",
    "    - overall_accuracy: Subset accuracy (exact match ratio)\n",
    "    - label_accuracies: List of accuracies for each individual label\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_pred[y_pred > 0.5] = 1\n",
    "    y_pred[y_pred < 0.5] = 0\n",
    "\n",
    "    # Ensure the shapes are the same\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(\"Shapes of y_true and y_pred must be the same.\")\n",
    "    \n",
    "    label_accuracies = np.mean(y_true == y_pred, axis=0).round(4)\n",
    "    overall_accuracy = np.mean(label_accuracies).round(4)\n",
    "\n",
    "    return overall_accuracy, label_accuracies\n",
    "\n",
    "multilabel_accuracy(pred, gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
